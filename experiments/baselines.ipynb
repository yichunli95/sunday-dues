{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11785 experiments.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHCvJRUbCkRu"
      },
      "source": [
        "!wget https://memexqa.cs.cmu.edu/fvta_model_zoo/prepro_v1.1.tgz\n",
        "!gunzip prepro_v1.1.tgz\n",
        "!tar -xvf prepro_v1.1.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E71TEL6SWs4A"
      },
      "source": [
        "# new_dataset_checked_by_hongyuan.py\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from torch.nn.utils.rnn import *\r\n",
        "from torch.utils.data import Dataset\r\n",
        "import itertools\r\n",
        "\r\n",
        "train_data = pd.read_pickle('prepro_v1.1/train_data.p')\r\n",
        "train_shared = pd.read_pickle('prepro_v1.1/train_shared.p')\r\n",
        "val_data = pd.read_pickle('prepro_v1.1/val_data.p')\r\n",
        "\r\n",
        "q_types = [\"when\", \"what\", \"who\", \"where\", \"how\"]\r\n",
        "qtype2qid = {}\r\n",
        "qtype2qid[\"when\"] = []\r\n",
        "qtype2qid[\"what\"] = []\r\n",
        "qtype2qid[\"who\"] = []\r\n",
        "qtype2qid[\"where\"] = []\r\n",
        "qtype2qid[\"how\"] = []\r\n",
        "\r\n",
        "for i, qid in enumerate(train_data['qid']):\r\n",
        "    if train_data['q'][i][0].lower() == \"when\":\r\n",
        "        qtype2qid[\"when\"].append(qid)\r\n",
        "    elif train_data['q'][i][0].lower() == \"what\":\r\n",
        "        qtype2qid[\"what\"].append(qid)\r\n",
        "    elif train_data['q'][i][0].lower() == \"who\":\r\n",
        "        qtype2qid[\"who\"].append(qid)\r\n",
        "    elif train_data['q'][i][0].lower() == \"where\":\r\n",
        "        qtype2qid[\"where\"].append(qid)\r\n",
        "    elif train_data['q'][i][0].lower() == \"how\":\r\n",
        "        qtype2qid[\"how\"].append(qid)\r\n",
        "\r\n",
        "for i, qid in enumerate(val_data['qid']):\r\n",
        "    if val_data['q'][i][0].lower() == \"when\":\r\n",
        "        qtype2qid[\"when\"].append(qid)\r\n",
        "    elif val_data['q'][i][0].lower() == \"what\":\r\n",
        "        qtype2qid[\"what\"].append(qid)\r\n",
        "    elif val_data['q'][i][0].lower() == \"who\":\r\n",
        "        qtype2qid[\"who\"].append(qid)\r\n",
        "    elif val_data['q'][i][0].lower() == \"where\":\r\n",
        "        qtype2qid[\"where\"].append(qid)\r\n",
        "    elif val_data['q'][i][0].lower() == \"how\":\r\n",
        "        qtype2qid[\"how\"].append(qid)\r\n",
        "\r\n",
        "q_lens = [len(q) for q in train_data['q']]\r\n",
        "cs_lens = [[len(c) for c in cs] for cs in train_data['cs']]\r\n",
        "cs_lens = list(itertools.chain(*cs_lens))\r\n",
        "y_lens = [len(y) for y in train_data['y']]\r\n",
        "photo_lens = [len(train_shared['albums'][aid]['photo_ids']) for aid in train_shared['albums']]\r\n",
        "all_photos_lens = [sum(len(train_shared['albums'][aid]['photo_ids']) for aid in aid_list) for aid_list in train_data['aid']]\r\n",
        "pts_lens = [len(pt) for aid in train_shared['albums'] for pt in train_shared['albums'][aid]['photo_titles']]\r\n",
        "when_lens = [len(train_shared['albums'][aid]['when']) for aid in train_shared['albums']]\r\n",
        "where_lens = [len(train_shared['albums'][aid]['where']) for aid in train_shared['albums']]\r\n",
        "album_title_lens = [len(train_shared['albums'][aid]['title']) for aid in train_shared['albums']]\r\n",
        "album_desc_lens = [len(train_shared['albums'][aid]['description']) for aid in train_shared['albums']]\r\n",
        "\r\n",
        "Q_THRES = int(np.percentile(q_lens, 90)) # 10\r\n",
        "Y_THRES = int(np.percentile(cs_lens, 90)) # 3, same as np.percentile(y_lens, 90)\r\n",
        "PTS_THRES = int(np.percentile(pts_lens, 90)) # 8\r\n",
        "WHEN_THRES = int(np.percentile(when_lens, 90)) # 4\r\n",
        "WHERE_THRES = int(np.percentile(where_lens, 90)) # 5\r\n",
        "PHOTOS_PER_ALBUM = int(np.percentile(photo_lens, 90)) # 10\r\n",
        "ALL_PHOTOS_THRES = max(all_photos_lens) # 72\r\n",
        "ALBUM_TITLE_THRES = int(np.percentile(album_title_lens, 90)) # 8\r\n",
        "ALBUM_DESC_THRES = int(np.percentile(album_desc_lens, 50)) # 11\r\n",
        "\r\n",
        "def train_collate(batch):\r\n",
        "    X, Y = zip(*batch)\r\n",
        "    q_vec = []\r\n",
        "    cs_vec = []\r\n",
        "    desc_vec = []\r\n",
        "    img_feats = []\r\n",
        "    q_len = []\r\n",
        "    cs0_len = []\r\n",
        "    cs1_len = []\r\n",
        "    cs2_len = []\r\n",
        "    cs3_len = []\r\n",
        "    desc_len = []\r\n",
        "    img_len = []\r\n",
        "    qid = []\r\n",
        "    pid = []\r\n",
        "    new_X = {}\r\n",
        "    for x in X:\r\n",
        "      q_len.append(x['q_len'])\r\n",
        "      cs0_len.append(x['cs_lens'][0])\r\n",
        "      cs1_len.append(x['cs_lens'][1])\r\n",
        "      cs2_len.append(x['cs_lens'][2])\r\n",
        "      cs3_len.append(x['cs_lens'][3])\r\n",
        "      desc_len.append(x['desc_len'])\r\n",
        "      img_len.append(x['img_len'])\r\n",
        "      q_vec.append(x['q_vec'])\r\n",
        "      cs_vec.append(x['cs_vec']) # x['cs_vec'] expected shape: <=Y_THRES, 4, 100\r\n",
        "      desc_vec.append(x['desc_vec'])\r\n",
        "      img_feats.append(x['img_feats'])\r\n",
        "      qid.append(x['qid'])\r\n",
        "      pid.append(x['pids'])\r\n",
        "\r\n",
        "    new_X['q_len'] = torch.LongTensor(q_len)\r\n",
        "    new_X['cs0_lens'] = torch.LongTensor(cs0_len)\r\n",
        "    new_X['cs1_lens'] = torch.LongTensor(cs1_len)\r\n",
        "    new_X['cs2_lens'] = torch.LongTensor(cs2_len)\r\n",
        "    new_X['cs3_lens'] = torch.LongTensor(cs3_len)\r\n",
        "    new_X['desc_len'] = torch.LongTensor(desc_len)\r\n",
        "    new_X['img_len'] = torch.LongTensor(img_len)\r\n",
        "    new_X['q_vec'] = pad_sequence(q_vec, batch_first=False, padding_value=0)  # T, B, 100 \r\n",
        "    new_X['cs_vec'] = pad_sequence(cs_vec, batch_first=False, padding_value=0)  # B, <=Y_THRES, 4, 100 -> T, B, 4, 100\r\n",
        "    new_X['desc_vec'] = pad_sequence(desc_vec, batch_first=False, padding_value=0)  # T, B, total_cat_len * 100\r\n",
        "    new_X['img_feats'] = pad_sequence(img_feats, batch_first=False, padding_value=0)  # T, B, 2537\r\n",
        "\r\n",
        "    return new_X, torch.LongTensor(Y), qid, pid\r\n",
        "\r\n",
        "class MemexQA_new(Dataset):\r\n",
        "    def __init__(self, data, shared):\r\n",
        "        # self.data keys -> ['q', 'idxs', 'cy', 'ccs', 'qid', 'y', 'aid', 'cq', 'yidx', 'cs']\r\n",
        "        # self.shared keys -> ['albums', 'pid2feat', 'word2vec', 'charCounter', 'wordCounter']\r\n",
        "        self.data = data\r\n",
        "        self.shared = shared\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.data['q'])\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        returned_item = {}\r\n",
        "\r\n",
        "        returned_item['qid'] = self.data['qid'][idx]\r\n",
        "\r\n",
        "        q = self.data['q'][idx]\r\n",
        "        # missing glove word-> [0] * 100 embedding\r\n",
        "        q_vec = torch.FloatTensor([self.shared['word2vec'][word.lower()] if word.lower() in self.shared['word2vec'] else [0] * 100 for word in q])\r\n",
        "        q_vec = q_vec[:Q_THRES]\r\n",
        "        returned_item['q_vec'] = q_vec  # largest possible shape: Q_THRES * 100\r\n",
        "        returned_item['q_len'] = q_vec.shape[0] \r\n",
        "\r\n",
        "        wrong_cs = self.data['cs'][idx]\r\n",
        "        correct_c = self.data['y'][idx]\r\n",
        "        yidx = self.data['yidx'][idx]\r\n",
        "        if yidx == 0:\r\n",
        "            cs = [correct_c] + wrong_cs\r\n",
        "        elif yidx == 1:\r\n",
        "            cs = wrong_cs[:1] + [correct_c] + wrong_cs[1:]\r\n",
        "        elif yidx == 2:\r\n",
        "            cs = wrong_cs[:2] + [correct_c] + wrong_cs[2:]\r\n",
        "        else:  # yidx == 3\r\n",
        "            cs = wrong_cs + [correct_c]\r\n",
        "        cs_vec = [[self.shared['word2vec'][word.lower()] if word.lower() in self.shared['word2vec'] else [0] * 100 for word in c] for c in cs]\r\n",
        "        cs_vec = [torch.FloatTensor(c[:Y_THRES]) for c in cs_vec]\r\n",
        "        cs_lens = [min(Y_THRES, len(c)) for c in cs_vec]\r\n",
        "        returned_item['cs_vec'] = pad_sequence(cs_vec, batch_first = False)  # largest possible shape: 4, Y_THRES, 100 ->  Y_THRES, 4, 100\r\n",
        "        returned_item['cs_lens'] = cs_lens\r\n",
        "\r\n",
        "        # aid: description + title + when + where + photo_titles\r\n",
        "        aid_list = self.data['aid'][idx]\r\n",
        "        pts_descs = []  # photo-level text features\r\n",
        "        pid_features = []  # photo-level img features from pre-trained CNN\r\n",
        "        pids = []\r\n",
        "        # for each album\r\n",
        "        total_cat_len = ALBUM_TITLE_THRES + ALBUM_DESC_THRES + WHEN_THRES + PTS_THRES + WHERE_THRES  # 8 + 11 + 4 + 8 + 5 = 36\r\n",
        "        for aid in aid_list:\r\n",
        "            album = self.shared['albums'][aid]\r\n",
        "            pids.extend(album['photo_ids'])\r\n",
        "            pts = album['photo_titles']  # all photo titles/aid\r\n",
        "            # concatenate album description, album title, album when and album where\r\n",
        "            desc = album['description'][:ALBUM_DESC_THRES] + album['title'][:ALBUM_TITLE_THRES] + album['when'][:WHEN_THRES] + album['where'][:WHERE_THRES]\r\n",
        "            for pt in pts:\r\n",
        "                photo_info = desc + pt[:PTS_THRES]\r\n",
        "                # largest possible shape: total_cat_len, 100\r\n",
        "                photo_info_vec = [self.shared['word2vec'][word.lower()] if word.lower() in self.shared['word2vec'] else [0] * 100 for word in photo_info]\r\n",
        "                if len(photo_info_vec) < total_cat_len:\r\n",
        "                    photo_info_vec = photo_info_vec + [[0] * 100 for _ in range(total_cat_len - len(photo_info_vec))]  # total_cat_len, 100\r\n",
        "                pts_descs.append(photo_info_vec)  # total number of photos (varies), total_cat_len, 100\r\n",
        "\r\n",
        "            for pid in self.shared['albums'][aid]['photo_ids']:\r\n",
        "                # img_feats\r\n",
        "                pid_features.append(self.shared['pid2feat'][pid])  # total number of photos (varies) * 2537\r\n",
        "\r\n",
        "        desc_vec = torch.FloatTensor(pts_descs).view(-1, total_cat_len * 100)  # total number of photos (varies), total_cat_len * 100\r\n",
        "        returned_item['pids'] = pids\r\n",
        "        returned_item['desc_vec'] = desc_vec\r\n",
        "        returned_item['desc_len'] = desc_vec.shape[0]  # total number of photos (varies)\r\n",
        "        img_feats_vec = torch.FloatTensor(pid_features)  # total number of photos (varies), 2537; NEWLY CHANGED (no matter what, it will vary; keep consistent with desc_vec)\r\n",
        "        returned_item['img_feats'] = img_feats_vec\r\n",
        "        returned_item['img_len'] = img_feats_vec.shape[0]  # total number of photos (varies)\r\n",
        "        return returned_item, yidx"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaK-U5E6C64q"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import *\n",
        "import pandas as pd\n",
        "\n",
        "class SimpleLSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, batch_size, num_layers, device, rnn_type = 'bilstm'):\n",
        "        super(SimpleLSTMModel, self).__init__()\n",
        "        self.device = device\n",
        "        self.img_feats_reshape = nn.Linear(2537, 100) # T, B, 100\n",
        "        self.desc_reshape = nn.Linear(3600, 100)  # T, B 100\n",
        "        self.flat = nn.Flatten()\n",
        "        if (rnn_type == 'bilstm'):\n",
        "            self.rnn_q = nn.LSTM(input_size, hidden_size, num_layers, batch_first = False, bidirectional = True)\n",
        "            self.rnn_c = nn.LSTM(input_size, hidden_size, num_layers, batch_first = False, bidirectional = True)\n",
        "            self.rnn_pt = nn.LSTM(input_size, hidden_size, num_layers, batch_first = False, bidirectional = True)\n",
        "            self.rnn_ps = nn.LSTM(input_size, hidden_size, num_layers, batch_first = False, bidirectional = True)\n",
        "        self.output = nn.Linear(4 * num_layers * (2 if rnn_type == 'bilstm' else 1) * hidden_size, 1)  # 4 * hidden size is the concatenated dim\n",
        "            \n",
        "\n",
        "    def forward(self, X):\n",
        "        # X is a list of dictionaries: 'q_vec', 'cs_vec', 'pts_vec', 'img_feats'\n",
        "        img_feats = X['img_feats'].to(self.device)\n",
        "        img_feats = self.img_feats_reshape(img_feats)  # T, B, 100\n",
        "        pts_vec = X['desc_vec'].to(self.device)\n",
        "        pts_vec = self.desc_reshape(pts_vec)  # T, B, 100\n",
        "        q_vec = X['q_vec'].to(self.device)  # T, B, 100\n",
        "        cs_vec = X['cs_vec'].to(self.device) # T, B, 4, 100\n",
        "        \n",
        "        packed_q_vec = pack_padded_sequence(q_vec, X['q_len'], enforce_sorted = False)\n",
        "        packed_c1_vec = pack_padded_sequence(cs_vec[:, :, 0, :], X['cs0_lens'], enforce_sorted = False)\n",
        "        packed_c2_vec = pack_padded_sequence(cs_vec[:, :, 1, :], X['cs1_lens'], enforce_sorted = False)\n",
        "        packed_c3_vec = pack_padded_sequence(cs_vec[:, :, 2, :], X['cs2_lens'], enforce_sorted = False)\n",
        "        packed_c4_vec = pack_padded_sequence(cs_vec[:, :, 3, :], X['cs3_lens'], enforce_sorted = False)\n",
        "        packed_pt_vec = pack_padded_sequence(pts_vec, X['desc_len'], enforce_sorted = False)\n",
        "        packed_img_vec = pack_padded_sequence(img_feats, X['img_len'], enforce_sorted=False)\n",
        "        _, (lstm_hidden_q, __) = self.rnn_q(packed_q_vec)  # num_layers * 2, B, hidden_size\n",
        "        _, (lstm_hidden_c1, __) = self.rnn_c(packed_c1_vec)\n",
        "        _, (lstm_hidden_c2, __) = self.rnn_c(packed_c2_vec)\n",
        "        _, (lstm_hidden_c3, __) = self.rnn_c(packed_c3_vec)\n",
        "        _, (lstm_hidden_c4, __) = self.rnn_c(packed_c4_vec)\n",
        "        _, (lstm_hidden_pt, __) = self.rnn_pt(packed_pt_vec)\n",
        "        _, (lstm_hidden_ps, __) = self.rnn_ps(img_feats)\n",
        "        lstm_hidden_cs = [lstm_hidden_c1, lstm_hidden_c2, lstm_hidden_c3, lstm_hidden_c4]\n",
        "      \n",
        "        concat_vec = torch.FloatTensor().to(self.device)\n",
        "        for i in range(4):\n",
        "            vec_to_be_cat = torch.cat((lstm_hidden_q, lstm_hidden_ps, lstm_hidden_cs[i], lstm_hidden_pt), dim = 0) # 4 * num_layers * 2, B, hidden_size\n",
        "            vec_to_be_cat = vec_to_be_cat.permute(1, 0, 2)  # B, 4 * num_layers * 2, hidden_size\n",
        "            vec_to_be_cat = torch.flatten(vec_to_be_cat, 1, 2)  # B, 4 * num_layers * 2 * hidden_size\n",
        "            vec_to_be_cat = torch.unsqueeze(vec_to_be_cat, 1)  # B, 1, 4 * num_layers * 2 * hidden_size\n",
        "            concat_vec = torch.cat((concat_vec, vec_to_be_cat), dim = 1) # B, 4, 4 * num_layers * 2 * hidden_size\n",
        "        out = self.output(concat_vec)  # B, 4, 1\n",
        "        out = out.squeeze(2)  # B, 4\n",
        "        return out"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH4dtq_iJOF9"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import *\n",
        "\n",
        "max_photo_len = max(all_photos_lens)\n",
        "max_q_len = max(q_lens)\n",
        "max_cs_len = max(cs_lens)\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.device = device\n",
        "        self.img_feats_reshape = nn.Linear(2537, 100)\n",
        "        self.desc_reshape = nn.Linear(3600, 100)\n",
        "        self.flat = nn.Flatten()\n",
        "        self.output = nn.Linear((2 * max_photo_len + max_q_len + max_cs_len) * 100, 1)  # TODO\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X is a list of dictionaries: 'q_vec', 'cs_vec', 'pts_vec', 'img_feats'\n",
        "        img_feats = X['img_feats'].to(self.device)\n",
        "        img_feats = self.img_feats_reshape(img_feats) # T, B, 100\n",
        "        img_feats = F.pad(img_feats, [0, 0, 0, 0, 0, max_photo_len - img_feats.shape[0]])\n",
        "        pts_vec = X['desc_vec'].to(self.device)\n",
        "        pts_vec = self.desc_reshape(pts_vec)  # T, B, 100\n",
        "        pts_vec = F.pad(pts_vec, [0, 0, 0, 0, 0, max_photo_len - pts_vec.shape[0]])\n",
        "        q_vec = X['q_vec'].to(self.device)  # T, B, 100\n",
        "        q_vec = F.pad(q_vec, [0, 0, 0, 0, 0, max_q_len - q_vec.shape[0]])\n",
        "        cs_vec = X['cs_vec'].to(self.device) # T, B, 4, 100\n",
        "        cs_vec = F.pad(cs_vec, [0, 0, 0, 0, 0, 0, 0, max_cs_len - cs_vec.shape[0]])\n",
        "        concat_vec = torch.FloatTensor().to(self.device)\n",
        "        for i in range(4):\n",
        "            vec_to_be_cat = torch.unsqueeze(self.flat(torch.cat((q_vec, img_feats, cs_vec[:, :, i, :], pts_vec), dim = 0).permute(1, 0, 2)), 1)  # B, 1, T * 100\n",
        "            concat_vec = torch.cat((concat_vec, vec_to_be_cat), dim = 1)\n",
        "        # len(X), 4, (dataset.Q_THRES + dataset.CS_THRES + dataset.PTS_TOTAL_THRES + dataset.PS_THRES)* 100\n",
        "        out = self.output(concat_vec)  # B, 4, 1\n",
        "        out = out.squeeze(2)\n",
        "        return out"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edxMeG-AW3YG"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "import torch.nn as nn\r\n",
        "import torch\r\n",
        "import pandas as pd\r\n",
        "import time\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import argparse\r\n",
        "import csv\r\n",
        "\r\n",
        "# hyperparams\r\n",
        "EPOCHS = 10\r\n",
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "# optimizer-related\r\n",
        "MOMENTUM = 1e-2\r\n",
        "LR = 1e-2\r\n",
        "LR_STEPSIZE = 5\r\n",
        "LR_DECAY = 0.85\r\n",
        "WD = 5e-6\r\n",
        "\r\n",
        "def main(train_data_pth, train_shared_pth, val_data_pth, val_shared_pth, test_data_pth, test_shared_pth, isTrain):\r\n",
        "    cuda = torch.cuda.is_available()\r\n",
        "    num_workers = 8 if cuda else 0\r\n",
        "    print(\"Loading data......\")\r\n",
        "    start = time.time()\r\n",
        "\r\n",
        "    train_shared = pd.read_pickle(train_shared_pth)\r\n",
        "    # random initial embedding matrix for new words\r\n",
        "    nonglove_dict = {word: np.random.normal(0, 1, 100) for word in train_shared['wordCounter'] if word not in train_shared['word2vec']}\r\n",
        "    train_shared['word2vec'].update(nonglove_dict)\r\n",
        "    \r\n",
        "    val_shared = pd.read_pickle(val_shared_pth)\r\n",
        "    val_nonglove_dict = {word: np.random.normal(0, 1, 100) for word in val_shared['wordCounter'] if word not in val_shared['word2vec']}\r\n",
        "    val_shared['word2vec'].update(val_nonglove_dict)\r\n",
        "\r\n",
        "    test_shared = pd.read_pickle(test_shared_pth)\r\n",
        "    test_nonglove_dict = {word: np.random.normal(0, 1, 100) for word in test_shared['wordCounter'] if word not in test_shared['word2vec']}\r\n",
        "    test_shared['word2vec'].update(test_nonglove_dict)\r\n",
        "\r\n",
        "    train_data = MemexQA_new(data=pd.read_pickle(train_data_pth), shared=train_shared)\r\n",
        "    valid_data = MemexQA_new(data=pd.read_pickle(val_data_pth), shared=val_shared)\r\n",
        "    test_data = MemexQA_new(data=pd.read_pickle(test_data_pth), shared=test_shared)\r\n",
        "\r\n",
        "    train_loader_args = dict(shuffle=True, batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\r\n",
        "        else dict(shuffle=True, batch_size=BATCH_SIZE, collate_fn=train_collate)\r\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, **train_loader_args)\r\n",
        "\r\n",
        "    valid_loader_args = dict(shuffle=False, batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\r\n",
        "        else dict(shuffle=False, batch_size=BATCH_SIZE, collate_fn=train_collate)\r\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_data, **valid_loader_args)\r\n",
        "\r\n",
        "    test_loader_args = dict(shuffle=False, batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\r\n",
        "        else dict(shuffle=False, batch_size=BATCH_SIZE, collate_fn=train_collate)  # TODO: test_collate\r\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, **test_loader_args)\r\n",
        "    print(f\"Loading data took {time.time() - start:.1f} seconds\")\r\n",
        "    \r\n",
        "    # initialize model\r\n",
        "    device = torch.device(\"cuda\" if cuda else \"cpu\")\r\n",
        "    \r\n",
        "    #model = NewFusionModel(100, 3600,2537,128, 2, 2, device, 64, 64, 4, 3, 1)\r\n",
        "    #model = NewLSTMModel(100, 3600, 2537, 128, device)\r\n",
        "    #q_cs_input_size, desc_input_size, img_input_size, hidden_size, linear_size, k1, s1, k2, s2, batch_size, device\r\n",
        "    model = SimpleLSTMModel(100, 128, 64, 2, device)\r\n",
        "    model.to(device)\r\n",
        "\r\n",
        "    # setup optim and loss\r\n",
        "    criterion = nn.CrossEntropyLoss()\r\n",
        "    optimizer= optim.Adam(model.parameters(), lr = LR, weight_decay= WD)\r\n",
        "    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_STEPSIZE, gamma=LR_DECAY)\r\n",
        "\r\n",
        "    # training\r\n",
        "    for i in range(EPOCHS):\r\n",
        "        start = time.time()\r\n",
        "        print(\"Starting training......\")\r\n",
        "        model.train()\r\n",
        "        n_correct, n_total = 0, 0\r\n",
        "        batch_count = 0\r\n",
        "        t_loss = 0\r\n",
        "        q_totals = [0] * 5\r\n",
        "        q_corrects = [0] * 5\r\n",
        "        for j, (batch_data, batch_labels, qids, pids) in enumerate(train_loader):\r\n",
        "            if j == len(train_loader) - 1:\r\n",
        "                break\r\n",
        "            batch_labels = batch_labels.long().to(device)\r\n",
        "            optimizer.zero_grad()\r\n",
        "            output = model(batch_data)\r\n",
        "            loss = criterion(output, batch_labels)\r\n",
        "            t_loss += loss.item()\r\n",
        "            res = torch.argmax(output, 1)\r\n",
        "            res = res.to(device)\r\n",
        "            for i, qid in enumerate(qids):\r\n",
        "                if qid in qtype2qid[\"when\"]:\r\n",
        "                    q_totals[0] += 1\r\n",
        "                    if batch_labels[i] == res[i]:\r\n",
        "                        q_corrects[0] += 1\r\n",
        "                if qid in qtype2qid[\"what\"]:\r\n",
        "                    q_totals[1] += 1\r\n",
        "                    if batch_labels[i] == res[i]:\r\n",
        "                        q_corrects[1] += 1\r\n",
        "                if qid in qtype2qid[\"who\"]:\r\n",
        "                    q_totals[2] += 1\r\n",
        "                    if batch_labels[i] == res[i]:\r\n",
        "                        q_corrects[2] += 1\r\n",
        "                if qid in qtype2qid[\"where\"]:\r\n",
        "                    q_totals[3] += 1\r\n",
        "                    if batch_labels[i] == res[i]:\r\n",
        "                        q_corrects[3] += 1\r\n",
        "                if qid in qtype2qid[\"how\"]:\r\n",
        "                    q_totals[4] += 1\r\n",
        "                    if batch_labels[i] == res[i]:\r\n",
        "                        q_corrects[4] += 1\r\n",
        "            n_correct += (res == batch_labels).sum().item()\r\n",
        "            n_total += batch_labels.shape[0]\r\n",
        "            batch_count += 1\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            #if batch_count % 20 == 19:\r\n",
        "            #    print(f\"correct choice:{batch_labels[:3]} , predicted choice: {res[:3]}\")\r\n",
        "        train_acc = n_correct / n_total\r\n",
        "        train_loss = t_loss / batch_count\r\n",
        "        print(f\"TRAIN ===> Epoch {i}, took time {time.time()-start:.1f}s, train accu: {train_acc:.4f}, train loss: {train_loss:.6f}\")\r\n",
        "        train_acc_q_type = np.array(q_corrects) /np.array(q_totals)\r\n",
        "        for i in range(5):  # 5 types of questions\r\n",
        "            print(\"TRAIN ACC \", q_types[i], \": \", train_acc_q_type[i])\r\n",
        "        #scheduler.step()\r\n",
        "        \r\n",
        "        # validate and save model \r\n",
        "        print(\"Start validation......\")\r\n",
        "        start = time.time()\r\n",
        "        with torch.no_grad():\r\n",
        "            model.eval()            \r\n",
        "            valid_correct, loss, num_of_batches, num_of_val = 0, 0, 0, 0\r\n",
        "            # validation for classification\r\n",
        "            q_totals = [0] * 5\r\n",
        "            q_corrects = [0] * 5\r\n",
        "            for j, (vb_data, vb_label, qids, pids) in enumerate(valid_loader):\r\n",
        "                if j == len(valid_loader) - 1:\r\n",
        "                    break\r\n",
        "                vb_label = vb_label.long().to(device)\r\n",
        "                v_output = model(vb_data)\r\n",
        "                resm = torch.argmax(v_output, 1)\r\n",
        "                resm = resm.to(device)\r\n",
        "                for i, qid in enumerate(qids):\r\n",
        "                    if qid in qtype2qid[\"when\"]:\r\n",
        "                        q_totals[0] += 1\r\n",
        "                        if vb_label[i] == resm[i]:\r\n",
        "                            q_corrects[0] += 1\r\n",
        "                    if qid in qtype2qid[\"what\"]:\r\n",
        "                        q_totals[1] += 1\r\n",
        "                        if vb_label[i] == resm[i]:\r\n",
        "                            q_corrects[1] += 1\r\n",
        "                    if qid in qtype2qid[\"who\"]:\r\n",
        "                        q_totals[2] += 1\r\n",
        "                        if vb_label[i] == resm[i]:\r\n",
        "                            q_corrects[2] += 1\r\n",
        "                    if qid in qtype2qid[\"where\"]:\r\n",
        "                        q_totals[3] += 1\r\n",
        "                        if vb_label[i] == resm[i]:\r\n",
        "                            q_corrects[3] += 1\r\n",
        "                    if qid in qtype2qid[\"how\"]:\r\n",
        "                        q_totals[4] += 1\r\n",
        "                        if vb_label[i] == resm[i]:\r\n",
        "                            q_corrects[4] += 1\r\n",
        "                correct = (resm == vb_label).sum().item()\r\n",
        "                valid_correct += correct\r\n",
        "                loss += criterion(v_output, vb_label).item()\r\n",
        "                num_of_batches += 1\r\n",
        "                num_of_val += vb_label.shape[0]\r\n",
        "                #if num_of_batches % 20 == 19:\r\n",
        "                #    print(f\"correct choice:{vb_label[:3]} , predicted choice: {resm[:3]}\")\r\n",
        "            val_loss = loss / num_of_batches\r\n",
        "            val_accu = valid_correct / num_of_val\r\n",
        "        print(f\"VALID ===> Epoch {i}, took time {time.time()-start:.1f}s, valid accu: {val_accu:.4f}, valid loss: {val_loss:.6f}\")\r\n",
        "        train_acc_q_type = np.array(q_corrects) /np.array(q_totals)\r\n",
        "        for i in range(5):  # 5 types of questions\r\n",
        "            print(\"VALID ACC \", q_types[i], \": \", train_acc_q_type[i])\r\n",
        "        \r\n",
        "        snapshot_prefix = os.path.join(os.getcwd(), 'snapshot/')\r\n",
        "        if not os.path.exists(snapshot_prefix):\r\n",
        "            os.makedirs(snapshot_prefix)\r\n",
        "        torch.save({\r\n",
        "                    'model_state_dict': model.state_dict(),\r\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\r\n",
        "                    #'scheduler_state_dict' : scheduler.state_dict(),\r\n",
        "        }, snapshot_prefix + \"Model_\"+str(i))\r\n",
        "    \r\n",
        "    # testing\r\n",
        "    if not isTrain:\r\n",
        "        print(\"Start testing......\")\r\n",
        "        start = time.time()\r\n",
        "        model.eval()\r\n",
        "        with torch.no_grad(), open('test_predictions.csv', 'w') as f:\r\n",
        "            writer = csv.writer(f, delimiter=',')\r\n",
        "            writer.writerow([\"predict\",\"actual\"])\r\n",
        "            for (tbatch_data, tbatch_data_labels) in test_loader:\r\n",
        "                test_out = model(tbatch_data)\r\n",
        "                predict = torch.argmax(test_out, axis=1)\r\n",
        "                correct = (predict == tbatch_data_labels).sum().item()\r\n",
        "                for (pred, actual) in zip(predict, correct):\r\n",
        "                    writer.writerow([pred, actual])\r\n",
        "        print(f\"Testing took {time.time()-start:.1f}s\")\r\n",
        "    print(\"Finished\")\r\n",
        "                \r\n",
        "                                                    \r\n",
        "    \r\n",
        "main('prepro_v1.1/train_data.p', 'prepro_v1.1/train_shared.p', \r\n",
        "     'prepro_v1.1/val_data.p', 'prepro_v1.1/val_shared.p', \r\n",
        "     'prepro_v1.1/test_data.p', 'prepro_v1.1/test_shared.p',\r\n",
        "     isTrain = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u36ceqR0zjBo"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "import torch.nn as nn\r\n",
        "import torch\r\n",
        "import pandas as pd\r\n",
        "import time\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import argparse\r\n",
        "import csv\r\n",
        "\r\n",
        "# hyperparams\r\n",
        "EPOCHS = 10\r\n",
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "# optimizer-related\r\n",
        "MOMENTUM = 1e-2\r\n",
        "LR = 1e-2\r\n",
        "LR_STEPSIZE = 5\r\n",
        "LR_DECAY = 0.85\r\n",
        "WD = 5e-6\r\n",
        "\r\n",
        "def main(train_data_pth, train_shared_pth, val_data_pth, val_shared_pth, test_data_pth, test_shared_pth, isTrain):\r\n",
        "    cuda = torch.cuda.is_available()\r\n",
        "    num_workers = 8 if cuda else 0\r\n",
        "    print(\"Loading data......\")\r\n",
        "    start = time.time()\r\n",
        "\r\n",
        "    train_shared = pd.read_pickle(train_shared_pth)\r\n",
        "    # random initial embedding matrix for new words\r\n",
        "    nonglove_dict = {word: np.random.normal(0, 1, 100) for word in train_shared['wordCounter'] if word not in train_shared['word2vec']}\r\n",
        "    train_shared['word2vec'].update(nonglove_dict)\r\n",
        "    \r\n",
        "    val_shared = pd.read_pickle(val_shared_pth)\r\n",
        "    val_nonglove_dict = {word: np.random.normal(0, 1, 100) for word in val_shared['wordCounter'] if word not in val_shared['word2vec']}\r\n",
        "    val_shared['word2vec'].update(val_nonglove_dict)\r\n",
        "\r\n",
        "    test_shared = pd.read_pickle(test_shared_pth)\r\n",
        "    test_nonglove_dict = {word: np.random.normal(0, 1, 100) for word in test_shared['wordCounter'] if word not in test_shared['word2vec']}\r\n",
        "    test_shared['word2vec'].update(test_nonglove_dict)\r\n",
        "\r\n",
        "    train_data = MemexQA_new(data=pd.read_pickle(train_data_pth), shared=train_shared)\r\n",
        "    valid_data = MemexQA_new(data=pd.read_pickle(val_data_pth), shared=val_shared)\r\n",
        "    test_data = MemexQA_new(data=pd.read_pickle(test_data_pth), shared=test_shared)\r\n",
        "\r\n",
        "    train_loader_args = dict(shuffle=True, batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\r\n",
        "        else dict(shuffle=True, batch_size=BATCH_SIZE, collate_fn=train_collate)\r\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, **train_loader_args)\r\n",
        "\r\n",
        "    valid_loader_args = dict(shuffle=False, batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\r\n",
        "        else dict(shuffle=False, batch_size=BATCH_SIZE, collate_fn=train_collate)\r\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_data, **valid_loader_args)\r\n",
        "\r\n",
        "    test_loader_args = dict(shuffle=False, batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\r\n",
        "        else dict(shuffle=False, batch_size=BATCH_SIZE, collate_fn=train_collate)  # TODO: test_collate\r\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, **test_loader_args)\r\n",
        "    print(f\"Loading data took {time.time() - start:.1f} seconds\")\r\n",
        "    \r\n",
        "    # initialize model\r\n",
        "    device = torch.device(\"cuda\" if cuda else \"cpu\")\r\n",
        "    \r\n",
        "    #model = NewFusionModel(100, 3600,2537,128, 2, 2, device, 64, 64, 4, 3, 1)\r\n",
        "    #model = NewLSTMModel(100, 3600, 2537, 128, device)\r\n",
        "    #q_cs_input_size, desc_input_size, img_input_size, hidden_size, linear_size, k1, s1, k2, s2, batch_size, device\r\n",
        "    #model = SimpleLSTMModel(100, 128, 64, 2, device)\r\n",
        "    model = LinearModel(device)\r\n",
        "    model.to(device)\r\n",
        "\r\n",
        "    # setup optim and loss\r\n",
        "    criterion = nn.CrossEntropyLoss()\r\n",
        "    optimizer= optim.Adam(model.parameters(), lr = LR, weight_decay= WD)\r\n",
        "    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_STEPSIZE, gamma=LR_DECAY)\r\n",
        "\r\n",
        "    # training\r\n",
        "    for i in range(EPOCHS):\r\n",
        "        start = time.time()\r\n",
        "        print(\"Starting training......\")\r\n",
        "        model.train()\r\n",
        "        n_correct, n_total = 0, 0\r\n",
        "        batch_count = 0\r\n",
        "        t_loss = 0\r\n",
        "        q_totals = [0] * 5\r\n",
        "        q_corrects = [0] * 5\r\n",
        "        for j, (batch_data, batch_labels, qids, pids) in enumerate(train_loader):\r\n",
        "            if j == len(train_loader) - 1:\r\n",
        "                break\r\n",
        "            batch_labels = batch_labels.long().to(device)\r\n",
        "            optimizer.zero_grad()\r\n",
        "            output = model(batch_data)\r\n",
        "            loss = criterion(output, batch_labels)\r\n",
        "            t_loss += loss.item()\r\n",
        "            res = torch.argmax(output, 1)\r\n",
        "            res = res.to(device)\r\n",
        "            for i, qid in enumerate(qids):\r\n",
        "                if qid in qtype2qid[\"when\"]:\r\n",
        "                    q_totals[0] += 1\r\n",
        "                    if batch_labels[i] == res[i]:\r\n",
        "                        q_corrects[0] += 1\r\n",
        "                if qid in qtype2qid[\"what\"]:\r\n",
        "                    q_totals[1] += 1\r\n",
        "                    if batch_labels[i] == res[i]:\r\n",
        "                        q_corrects[1] += 1\r\n",
        "                if qid in qtype2qid[\"who\"]:\r\n",
        "                    q_totals[2] += 1\r\n",
        "                    if batch_labels[i] == res[i]:\r\n",
        "                        q_corrects[2] += 1\r\n",
        "                if qid in qtype2qid[\"where\"]:\r\n",
        "                    q_totals[3] += 1\r\n",
        "                    if batch_labels[i] == res[i]:\r\n",
        "                        q_corrects[3] += 1\r\n",
        "                if qid in qtype2qid[\"how\"]:\r\n",
        "                    q_totals[4] += 1\r\n",
        "                    if batch_labels[i] == res[i]:\r\n",
        "                        q_corrects[4] += 1\r\n",
        "            n_correct += (res == batch_labels).sum().item()\r\n",
        "            n_total += batch_labels.shape[0]\r\n",
        "            batch_count += 1\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            #if batch_count % 20 == 19:\r\n",
        "            #    print(f\"correct choice:{batch_labels[:3]} , predicted choice: {res[:3]}\")\r\n",
        "        train_acc = n_correct / n_total\r\n",
        "        train_loss = t_loss / batch_count\r\n",
        "        print(f\"TRAIN ===> Epoch {i}, took time {time.time()-start:.1f}s, train accu: {train_acc:.4f}, train loss: {train_loss:.6f}\")\r\n",
        "        train_acc_q_type = np.array(q_corrects) /np.array(q_totals)\r\n",
        "        for i in range(5):  # 5 types of questions\r\n",
        "            print(\"TRAIN ACC \", q_types[i], \": \", train_acc_q_type[i])\r\n",
        "        #scheduler.step()\r\n",
        "        \r\n",
        "        # validate and save model \r\n",
        "        print(\"Start validation......\")\r\n",
        "        start = time.time()\r\n",
        "        with torch.no_grad():\r\n",
        "            model.eval()            \r\n",
        "            valid_correct, loss, num_of_batches, num_of_val = 0, 0, 0, 0\r\n",
        "            # validation for classification\r\n",
        "            q_totals = [0] * 5\r\n",
        "            q_corrects = [0] * 5\r\n",
        "            for j, (vb_data, vb_label, qids, pids) in enumerate(valid_loader):\r\n",
        "                if j == len(valid_loader) - 1:\r\n",
        "                    break\r\n",
        "                vb_label = vb_label.long().to(device)\r\n",
        "                v_output = model(vb_data)\r\n",
        "                resm = torch.argmax(v_output, 1)\r\n",
        "                resm = resm.to(device)\r\n",
        "                for i, qid in enumerate(qids):\r\n",
        "                    if qid in qtype2qid[\"when\"]:\r\n",
        "                        q_totals[0] += 1\r\n",
        "                        if vb_label[i] == resm[i]:\r\n",
        "                            q_corrects[0] += 1\r\n",
        "                    if qid in qtype2qid[\"what\"]:\r\n",
        "                        q_totals[1] += 1\r\n",
        "                        if vb_label[i] == resm[i]:\r\n",
        "                            q_corrects[1] += 1\r\n",
        "                    if qid in qtype2qid[\"who\"]:\r\n",
        "                        q_totals[2] += 1\r\n",
        "                        if vb_label[i] == resm[i]:\r\n",
        "                            q_corrects[2] += 1\r\n",
        "                    if qid in qtype2qid[\"where\"]:\r\n",
        "                        q_totals[3] += 1\r\n",
        "                        if vb_label[i] == resm[i]:\r\n",
        "                            q_corrects[3] += 1\r\n",
        "                    if qid in qtype2qid[\"how\"]:\r\n",
        "                        q_totals[4] += 1\r\n",
        "                        if vb_label[i] == resm[i]:\r\n",
        "                            q_corrects[4] += 1\r\n",
        "                correct = (resm == vb_label).sum().item()\r\n",
        "                valid_correct += correct\r\n",
        "                loss += criterion(v_output, vb_label).item()\r\n",
        "                num_of_batches += 1\r\n",
        "                num_of_val += vb_label.shape[0]\r\n",
        "                #if num_of_batches % 20 == 19:\r\n",
        "                #    print(f\"correct choice:{vb_label[:3]} , predicted choice: {resm[:3]}\")\r\n",
        "            val_loss = loss / num_of_batches\r\n",
        "            val_accu = valid_correct / num_of_val\r\n",
        "        print(f\"VALID ===> Epoch {i}, took time {time.time()-start:.1f}s, valid accu: {val_accu:.4f}, valid loss: {val_loss:.6f}\")\r\n",
        "        train_acc_q_type = np.array(q_corrects) /np.array(q_totals)\r\n",
        "        for i in range(5):  # 5 types of questions\r\n",
        "            print(\"VALID ACC \", q_types[i], \": \", train_acc_q_type[i])\r\n",
        "        \r\n",
        "        snapshot_prefix = os.path.join(os.getcwd(), 'snapshot/')\r\n",
        "        if not os.path.exists(snapshot_prefix):\r\n",
        "            os.makedirs(snapshot_prefix)\r\n",
        "        torch.save({\r\n",
        "                    'model_state_dict': model.state_dict(),\r\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\r\n",
        "                    #'scheduler_state_dict' : scheduler.state_dict(),\r\n",
        "        }, snapshot_prefix + \"Model_\"+str(i))\r\n",
        "    \r\n",
        "    # testing\r\n",
        "    if not isTrain:\r\n",
        "        print(\"Start testing......\")\r\n",
        "        start = time.time()\r\n",
        "        model.eval()\r\n",
        "        with torch.no_grad(), open('test_predictions.csv', 'w') as f:\r\n",
        "            writer = csv.writer(f, delimiter=',')\r\n",
        "            writer.writerow([\"predict\",\"actual\"])\r\n",
        "            for (tbatch_data, tbatch_data_labels) in test_loader:\r\n",
        "                test_out = model(tbatch_data)\r\n",
        "                predict = torch.argmax(test_out, axis=1)\r\n",
        "                correct = (predict == tbatch_data_labels).sum().item()\r\n",
        "                for (pred, actual) in zip(predict, correct):\r\n",
        "                    writer.writerow([pred, actual])\r\n",
        "        print(f\"Testing took {time.time()-start:.1f}s\")\r\n",
        "    print(\"Finished\")\r\n",
        "                \r\n",
        "                                                    \r\n",
        "    \r\n",
        "main('prepro_v1.1/train_data.p', 'prepro_v1.1/train_shared.p', \r\n",
        "     'prepro_v1.1/val_data.p', 'prepro_v1.1/val_shared.p', \r\n",
        "     'prepro_v1.1/test_data.p', 'prepro_v1.1/test_shared.p',\r\n",
        "     isTrain = True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}