{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11785 Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUmOgdr_EVkk",
        "outputId": "925038ff-8085-47c4-f47b-c86f9afe5757"
      },
      "source": [
        "!wget https://memexqa.cs.cmu.edu/fvta_model_zoo/prepro_v1.1.tgz\n",
        "!gunzip prepro_v1.1.tgz\n",
        "!tar -xvf prepro_v1.1.tar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-09 02:00:35--  https://memexqa.cs.cmu.edu/fvta_model_zoo/prepro_v1.1.tgz\n",
            "Resolving memexqa.cs.cmu.edu (memexqa.cs.cmu.edu)... 128.2.220.9\n",
            "Connecting to memexqa.cs.cmu.edu (memexqa.cs.cmu.edu)|128.2.220.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 212511000 (203M) [application/x-gzip]\n",
            "Saving to: ‘prepro_v1.1.tgz’\n",
            "\n",
            "prepro_v1.1.tgz     100%[===================>] 202.67M  30.8MB/s    in 18s     \n",
            "\n",
            "2020-12-09 02:00:53 (11.4 MB/s) - ‘prepro_v1.1.tgz’ saved [212511000/212511000]\n",
            "\n",
            "prepro_v1.1/\n",
            "prepro_v1.1/test_data.p\n",
            "prepro_v1.1/train_shared.p\n",
            "prepro_v1.1/test_shared.p\n",
            "prepro_v1.1/train_data.p\n",
            "prepro_v1.1/val_data.p\n",
            "prepro_v1.1/val_shared.p\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj23x6UIEoyN"
      },
      "source": [
        "# new_dataset_checked_by_hongyuan.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.utils.rnn import *\n",
        "from torch.utils.data import Dataset\n",
        "import itertools\n",
        "\n",
        "train_data = pd.read_pickle('prepro_v1.1/train_data.p')\n",
        "train_shared = pd.read_pickle('prepro_v1.1/train_shared.p')\n",
        "val_data = pd.read_pickle('prepro_v1.1/val_data.p')\n",
        "\n",
        "q_types = [\"when\", \"what\", \"who\", \"where\", \"how\"]\n",
        "qtype2qid = {}\n",
        "qtype2qid[\"when\"] = []\n",
        "qtype2qid[\"what\"] = []\n",
        "qtype2qid[\"who\"] = []\n",
        "qtype2qid[\"where\"] = []\n",
        "qtype2qid[\"how\"] = []\n",
        "\n",
        "for i, qid in enumerate(train_data['qid']):\n",
        "    if train_data['q'][i][0].lower() == \"when\":\n",
        "        qtype2qid[\"when\"].append(qid)\n",
        "    elif train_data['q'][i][0].lower() == \"what\":\n",
        "        qtype2qid[\"what\"].append(qid)\n",
        "    elif train_data['q'][i][0].lower() == \"who\":\n",
        "        qtype2qid[\"who\"].append(qid)\n",
        "    elif train_data['q'][i][0].lower() == \"where\":\n",
        "        qtype2qid[\"where\"].append(qid)\n",
        "    elif train_data['q'][i][0].lower() == \"how\":\n",
        "        qtype2qid[\"how\"].append(qid)\n",
        "\n",
        "for i, qid in enumerate(val_data['qid']):\n",
        "    if val_data['q'][i][0].lower() == \"when\":\n",
        "        qtype2qid[\"when\"].append(qid)\n",
        "    elif val_data['q'][i][0].lower() == \"what\":\n",
        "        qtype2qid[\"what\"].append(qid)\n",
        "    elif val_data['q'][i][0].lower() == \"who\":\n",
        "        qtype2qid[\"who\"].append(qid)\n",
        "    elif val_data['q'][i][0].lower() == \"where\":\n",
        "        qtype2qid[\"where\"].append(qid)\n",
        "    elif val_data['q'][i][0].lower() == \"how\":\n",
        "        qtype2qid[\"how\"].append(qid)\n",
        "\n",
        "q_lens = [len(q) for q in train_data['q']]\n",
        "cs_lens = [[len(c) for c in cs] for cs in train_data['cs']]\n",
        "cs_lens = list(itertools.chain(*cs_lens))\n",
        "y_lens = [len(y) for y in train_data['y']]\n",
        "photo_lens = [len(train_shared['albums'][aid]['photo_ids']) for aid in train_shared['albums']]\n",
        "all_photos_lens = [sum(len(train_shared['albums'][aid]['photo_ids']) for aid in aid_list) for aid_list in train_data['aid']]\n",
        "pts_lens = [len(pt) for aid in train_shared['albums'] for pt in train_shared['albums'][aid]['photo_titles']]\n",
        "when_lens = [len(train_shared['albums'][aid]['when']) for aid in train_shared['albums']]\n",
        "where_lens = [len(train_shared['albums'][aid]['where']) for aid in train_shared['albums']]\n",
        "album_title_lens = [len(train_shared['albums'][aid]['title']) for aid in train_shared['albums']]\n",
        "album_desc_lens = [len(train_shared['albums'][aid]['description']) for aid in train_shared['albums']]\n",
        "\n",
        "Q_THRES = int(np.percentile(q_lens, 90)) # 10\n",
        "Y_THRES = int(np.percentile(cs_lens, 90)) # 3, same as np.percentile(y_lens, 90)\n",
        "PTS_THRES = int(np.percentile(pts_lens, 90)) # 8\n",
        "WHEN_THRES = int(np.percentile(when_lens, 90)) # 4\n",
        "WHERE_THRES = int(np.percentile(where_lens, 90)) # 5\n",
        "PHOTOS_PER_ALBUM = int(np.percentile(photo_lens, 90)) # 10\n",
        "ALL_PHOTOS_THRES = max(all_photos_lens) # 72\n",
        "ALBUM_TITLE_THRES = int(np.percentile(album_title_lens, 90)) # 8\n",
        "ALBUM_DESC_THRES = int(np.percentile(album_desc_lens, 50)) # 11\n",
        "\n",
        "def train_collate(batch):\n",
        "    X, Y = zip(*batch)\n",
        "    q_vec = []\n",
        "    cs_vec = []\n",
        "    desc_vec = []\n",
        "    img_feats = []\n",
        "    q_len = []\n",
        "    cs0_len = []\n",
        "    cs1_len = []\n",
        "    cs2_len = []\n",
        "    cs3_len = []\n",
        "    desc_len = []\n",
        "    img_len = []\n",
        "    qid = []\n",
        "    pid = []\n",
        "    new_X = {}\n",
        "    for x in X:\n",
        "      q_len.append(x['q_len'])\n",
        "      cs0_len.append(x['cs_lens'][0])\n",
        "      cs1_len.append(x['cs_lens'][1])\n",
        "      cs2_len.append(x['cs_lens'][2])\n",
        "      cs3_len.append(x['cs_lens'][3])\n",
        "      desc_len.append(x['desc_len'])\n",
        "      img_len.append(x['img_len'])\n",
        "      q_vec.append(x['q_vec'])\n",
        "      cs_vec.append(x['cs_vec']) # x['cs_vec'] expected shape: <=Y_THRES, 4, 100\n",
        "      desc_vec.append(x['desc_vec'])\n",
        "      img_feats.append(x['img_feats'])\n",
        "      qid.append(x['qid'])\n",
        "      pid.append(x['pids'])\n",
        "\n",
        "    new_X['q_len'] = torch.LongTensor(q_len)\n",
        "    new_X['cs0_lens'] = torch.LongTensor(cs0_len)\n",
        "    new_X['cs1_lens'] = torch.LongTensor(cs1_len)\n",
        "    new_X['cs2_lens'] = torch.LongTensor(cs2_len)\n",
        "    new_X['cs3_lens'] = torch.LongTensor(cs3_len)\n",
        "    new_X['desc_len'] = torch.LongTensor(desc_len)\n",
        "    new_X['img_len'] = torch.LongTensor(img_len)\n",
        "    new_X['q_vec'] = pad_sequence(q_vec, batch_first=False, padding_value=0)  # T, B, 100 \n",
        "    new_X['cs_vec'] = pad_sequence(cs_vec, batch_first=False, padding_value=0)  # B, <=Y_THRES, 4, 100 -> T, B, 4, 100\n",
        "    new_X['desc_vec'] = pad_sequence(desc_vec, batch_first=False, padding_value=0)  # T, B, total_cat_len * 100\n",
        "    new_X['img_feats'] = pad_sequence(img_feats, batch_first=False, padding_value=0)  # T, B, 2537\n",
        "\n",
        "    return new_X, torch.LongTensor(Y), qid, pid\n",
        "\n",
        "class MemexQA_new(Dataset):\n",
        "    def __init__(self, data, shared):\n",
        "        # self.data keys -> ['q', 'idxs', 'cy', 'ccs', 'qid', 'y', 'aid', 'cq', 'yidx', 'cs']\n",
        "        # self.shared keys -> ['albums', 'pid2feat', 'word2vec', 'charCounter', 'wordCounter']\n",
        "        self.data = data\n",
        "        self.shared = shared\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data['q'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        returned_item = {}\n",
        "\n",
        "        returned_item['qid'] = self.data['qid'][idx]\n",
        "\n",
        "        q = self.data['q'][idx]\n",
        "        # missing glove word-> [0] * 100 embedding\n",
        "        q_vec = torch.FloatTensor([self.shared['word2vec'][word.lower()] if word.lower() in self.shared['word2vec'] else [0] * 100 for word in q])\n",
        "        q_vec = q_vec[:Q_THRES]\n",
        "        returned_item['q_vec'] = q_vec  # largest possible shape: Q_THRES * 100\n",
        "        returned_item['q_len'] = q_vec.shape[0] \n",
        "\n",
        "        wrong_cs = self.data['cs'][idx]\n",
        "        correct_c = self.data['y'][idx]\n",
        "        yidx = self.data['yidx'][idx]\n",
        "        if yidx == 0:\n",
        "            cs = [correct_c] + wrong_cs\n",
        "        elif yidx == 1:\n",
        "            cs = wrong_cs[:1] + [correct_c] + wrong_cs[1:]\n",
        "        elif yidx == 2:\n",
        "            cs = wrong_cs[:2] + [correct_c] + wrong_cs[2:]\n",
        "        else:  # yidx == 3\n",
        "            cs = wrong_cs + [correct_c]\n",
        "        cs_vec = [[self.shared['word2vec'][word.lower()] if word.lower() in self.shared['word2vec'] else [0] * 100 for word in c] for c in cs]\n",
        "        cs_vec = [torch.FloatTensor(c[:Y_THRES]) for c in cs_vec]\n",
        "        cs_lens = [min(Y_THRES, len(c)) for c in cs_vec]\n",
        "        returned_item['cs_vec'] = pad_sequence(cs_vec, batch_first = False)  # largest possible shape: 4, Y_THRES, 100 ->  Y_THRES, 4, 100\n",
        "        returned_item['cs_lens'] = cs_lens\n",
        "\n",
        "        # aid: description + title + when + where + photo_titles\n",
        "        aid_list = self.data['aid'][idx]\n",
        "        pts_descs = []  # photo-level text features\n",
        "        pid_features = []  # photo-level img features from pre-trained CNN\n",
        "        pids = []\n",
        "        # for each album\n",
        "        total_cat_len = ALBUM_TITLE_THRES + ALBUM_DESC_THRES + WHEN_THRES + PTS_THRES + WHERE_THRES  # 8 + 11 + 4 + 8 + 5 = 36\n",
        "        for aid in aid_list:\n",
        "            album = self.shared['albums'][aid]\n",
        "            pids.extend(album['photo_ids'])\n",
        "            pts = album['photo_titles']  # all photo titles/aid\n",
        "            # concatenate album description, album title, album when and album where\n",
        "            desc = album['description'][:ALBUM_DESC_THRES] + album['title'][:ALBUM_TITLE_THRES] + album['when'][:WHEN_THRES] + album['where'][:WHERE_THRES]\n",
        "            for pt in pts:\n",
        "                photo_info = desc + pt[:PTS_THRES]\n",
        "                # largest possible shape: total_cat_len, 100\n",
        "                photo_info_vec = [self.shared['word2vec'][word.lower()] if word.lower() in self.shared['word2vec'] else [0] * 100 for word in photo_info]\n",
        "                if len(photo_info_vec) < total_cat_len:\n",
        "                    photo_info_vec = photo_info_vec + [[0] * 100 for _ in range(total_cat_len - len(photo_info_vec))]  # total_cat_len, 100\n",
        "                pts_descs.append(photo_info_vec)  # total number of photos (varies), total_cat_len, 100\n",
        "\n",
        "            for pid in self.shared['albums'][aid]['photo_ids']:\n",
        "                # img_feats\n",
        "                pid_features.append(self.shared['pid2feat'][pid])  # total number of photos (varies) * 2537\n",
        "\n",
        "        desc_vec = torch.FloatTensor(pts_descs).view(-1, total_cat_len * 100)  # total number of photos (varies), total_cat_len * 100\n",
        "        returned_item['pids'] = pids\n",
        "        returned_item['desc_vec'] = desc_vec\n",
        "        returned_item['desc_len'] = desc_vec.shape[0]  # total number of photos (varies)\n",
        "        img_feats_vec = torch.FloatTensor(pid_features)  # total number of photos (varies), 2537; NEWLY CHANGED (no matter what, it will vary; keep consistent with desc_vec)\n",
        "        returned_item['img_feats'] = img_feats_vec\n",
        "        returned_item['img_len'] = img_feats_vec.shape[0]  # total number of photos (varies)\n",
        "        return returned_item, yidx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18b-Wc-nFKX-"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import *\n",
        "import pandas as pd\n",
        "\n",
        "# crossEntropyLoss use ignore_index = 0\n",
        "\n",
        "\n",
        "# dims fed into LSTM: seq_len, batch_size, input_size\n",
        "# B, T, F\n",
        "# q_vec -> B, T, 100 (glove embedding)\n",
        "# cs_vec -> B, 4, T = Y_THRES, 100\n",
        "# desc_vec(prev. pt) -> B, T = all_photo_titles_albums * DESC_THRESH, 100\n",
        "# ps_vec -> B, T = num_of_albums * 3, 2537\n",
        "class NewFusionModel(nn.Module):\n",
        "    def __init__(self, q_cs_input_size, desc_input_size, img_input_size, hidden_size, batch_size,\n",
        "                num_layers, device, q_linear_size, img_linear_size, multimodal_out, kernel, stride = 1, rnn_type = 'bilstm'):\n",
        "        super(NewFusionModel, self).__init__()\n",
        "        self.device = device\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_size = batch_size\n",
        "        self.q_cs_input_size = q_cs_input_size   #input_size qvec and cs_vec\n",
        "        self.desc_input_size = desc_input_size\n",
        "        self.img_input_size = img_input_size\n",
        "        self.q_linear_size = q_linear_size # s1\n",
        "        self.img_linear_size = img_linear_size # s2\n",
        "        self.num_directions = 2 if rnn_type == 'bilstm' else 1\n",
        "        self.multimodal_out = multimodal_out\n",
        "        self.kernel = kernel\n",
        "        self.stride = stride\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        if (rnn_type == 'bilstm'):\n",
        "            self.rnn_q = nn.LSTM(q_cs_input_size, hidden_size, self.num_layers, batch_first = False, bidirectional = True) # questions\n",
        "            self.rnn_c = nn.LSTM(q_cs_input_size, hidden_size, self.num_layers, batch_first = False, bidirectional = True) # choices\n",
        "            self.rnn_desc = nn.LSTM(desc_input_size, hidden_size, self.num_layers, batch_first = False, bidirectional = True) # photo titles\n",
        "            self.rnn_ps = nn.LSTM(img_input_size, hidden_size, self.num_layers, batch_first = False, bidirectional = True) # image features\n",
        "        else:\n",
        "            self.rnn_q = nn.LSTM(q_cs_input_size, hidden_size, self.num_layers, batch_first = False, bidirectional = False)\n",
        "            self.rnn_c = nn.LSTM(q_cs_input_size, hidden_size, self.num_layers, batch_first = False, bidirectional = False)\n",
        "            self.rnn_desc = nn.LSTM(desc_input_size, hidden_size, self.num_layers, batch_first = False, bidirectional = False)\n",
        "            self.rnn_ps = nn.LSTM(img_input_size, hidden_size, self.num_layers, batch_first = False, bidirectional = False)\n",
        "\n",
        "        self.img_linear = nn.Linear(hidden_size, self.img_linear_size) \n",
        "        self.q_linear = nn.Linear(hidden_size, self.q_linear_size)\n",
        "\n",
        "        self.multimodal_cnn = nn.Conv1d(self.num_directions * self.num_layers, self.multimodal_out, self.kernel, self.stride)\n",
        "        # 2 * hidden_size if not passing in rnn_q hidden output, 3 * hidden_size if passing in \n",
        "        multimodal_cnn_in_size = 2 * hidden_size + self.num_layers * self.num_directions * batch_size * q_linear_size // self.img_linear_size\n",
        "        multimodal_cnn_out_size = (multimodal_cnn_in_size - self.kernel) // self.stride + 1\n",
        "        self.output = nn.Linear(self.multimodal_out * multimodal_cnn_out_size, 1)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        # X is a list of dictionaries: 'q_vec', 'cs_vec', 'desc_vec', 'img_feats'\n",
        "        # BATCH_SIZE = len(X)\n",
        "        # B, T, F\n",
        "        # q_vec -> B, T, 100 (glove embedding)\n",
        "        # cs_vec -> B, 4, T=Y_THRES, 100\n",
        "        # desc_vec(prev. pt) -> B, T=all_photo_titles_albums * 40, 100\n",
        "        # img_feats -> B, T=num_of_albums * 3, 2537\n",
        "\n",
        "\n",
        "        # q_vec = pad_sequence(X['q_vec'], batch_first=False, padding_value=0).to(self.device)  # question \n",
        "        # cs_vec = pad_sequence(X['cs_vec'].permute(0,2,1,3), batch_first = False, padding_value=0).to(self.device) # 4 choices T, B, 4, 100\n",
        "        # desc_vec = pad_sequence(X['desc_vec'], batch_first=False, padding_value=0).to(self.device) \n",
        "        # img_feats = pad_sequence(X['img_feats'], batch_first=False, padding_value=0).to(self.device)\n",
        "        q_vec = X['q_vec']\n",
        "        cs_vec = X['cs_vec']\n",
        "        desc_vec = X['desc_vec']\n",
        "        img_feats = X['img_feats'].to(self.device)\n",
        "        packed_q_vec = pack_padded_sequence(q_vec, X['q_len'], batch_first=False, enforce_sorted = False).to(self.device)\n",
        "        packed_c0_vec = pack_padded_sequence(cs_vec[:, :, 0, :], X['cs0_lens'], batch_first = False, enforce_sorted = False).to(self.device)\n",
        "        packed_c1_vec = pack_padded_sequence(cs_vec[:, :, 1, :], X['cs1_lens'], batch_first = False, enforce_sorted = False).to(self.device)\n",
        "        packed_c2_vec = pack_padded_sequence(cs_vec[:, :, 2, :], X['cs2_lens'], batch_first = False, enforce_sorted = False).to(self.device)\n",
        "        packed_c3_vec = pack_padded_sequence(cs_vec[:, :, 3, :], X['cs3_lens'], batch_first = False, enforce_sorted = False).to(self.device)\n",
        "        packed_pt_vec = pack_padded_sequence(desc_vec, X['desc_len'], batch_first = False, enforce_sorted = False).to(self.device)\n",
        "        #img_feats = img_feats.permute(1,0,2)  # batch_size, seq_len, input_size -> seq_len, batch_size, input_size\n",
        "        \n",
        "        #print(\"packed_c1_vec: \", packed_c1_vec.data.shape)\n",
        "        # hidden dims: num_layers * num_directions, batch_size, hidden_size\n",
        "        _, (lstm_hidden_q, __) = self.rnn_q(packed_q_vec)\n",
        "        _, (lstm_hidden_c0, __) = self.rnn_c(packed_c0_vec)\n",
        "        _, (lstm_hidden_c1, __) = self.rnn_c(packed_c1_vec)\n",
        "        _, (lstm_hidden_c2, __) = self.rnn_c(packed_c2_vec)\n",
        "        _, (lstm_hidden_c3, __) = self.rnn_c(packed_c3_vec)\n",
        "        _, (lstm_hidden_pt, __) = self.rnn_desc(packed_pt_vec)\n",
        "        _, (lstm_hidden_ps, __) = self.rnn_ps(img_feats)\n",
        "        lstm_hidden_cs = [lstm_hidden_c0, lstm_hidden_c1, lstm_hidden_c2, lstm_hidden_c3]\n",
        "        \n",
        "        candidate_weights = self.q_linear(lstm_hidden_q) # output: (num_direction * num_layers, batch_size, self.q_linear_size)\n",
        "        img_feats = self.img_linear(lstm_hidden_ps) # output: (num_direction * num_layers, batch_size, hidden_size)\n",
        "        # dyanmic parameter layer\n",
        "        dynamic_parameter_out = self.num_directions * self.num_layers * self.batch_size * self.q_linear_size // self.img_linear_size\n",
        "        dynamic_parameter_matrix = torch.flatten(candidate_weights)[:self.img_linear_size * dynamic_parameter_out]\n",
        "        dynamic_parameter_matrix = dynamic_parameter_matrix.reshape(self.img_linear_size, dynamic_parameter_out)\n",
        "        q_img_fused = img_feats @ dynamic_parameter_matrix\n",
        "\n",
        "        # multimodal cnn layer\n",
        "        cnn_out_list = []\n",
        "        for i in range(4):\n",
        "            vec = torch.cat((q_img_fused, lstm_hidden_cs[i], lstm_hidden_pt), dim = 2).to(self.device) \n",
        "            vec = vec.permute(1, 0, 2) # batch_size, num_direction * num_layers, 2 * hidden + dynamic_parameter_out\n",
        "            vec = self.multimodal_cnn(vec)\n",
        "            cnn_out_list.append(vec)\n",
        "        for i in range(4):\n",
        "            cnn_out_list[i] = torch.flatten(cnn_out_list[i], start_dim = 1).unsqueeze(1)\n",
        "        classification_input = torch.cat(cnn_out_list, dim = 1) # (batch_size, 4, out_ch * cnn_out)\n",
        "        logits = self.output(classification_input)\n",
        "        return logits.squeeze(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_rcBwDaOSLJ",
        "outputId": "027b4ffa-f641-411b-f871-fe58474a172d"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "import torch.nn as nn\r\n",
        "import torch\r\n",
        "import pandas as pd\r\n",
        "import time\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import argparse\r\n",
        "import csv\r\n",
        "\r\n",
        "# hyperparams\r\n",
        "EPOCHS = 10\r\n",
        "BATCH_SIZE = 64\r\n",
        "\r\n",
        "# optimizer-related\r\n",
        "MOMENTUM = 1e-2\r\n",
        "LR = 1e-2\r\n",
        "LR_STEPSIZE = 5\r\n",
        "LR_DECAY = 0.85\r\n",
        "WD = 5e-6\r\n",
        "\r\n",
        "def main(train_data_pth, train_shared_pth, val_data_pth, val_shared_pth, test_data_pth, test_shared_pth, isTrain):\r\n",
        "    cuda = torch.cuda.is_available()\r\n",
        "    num_workers = 8 if cuda else 0\r\n",
        "    print(\"Loading data......\")\r\n",
        "    start = time.time()\r\n",
        "\r\n",
        "    train_shared = pd.read_pickle(train_shared_pth)\r\n",
        "    # random initial embedding matrix for new words\r\n",
        "    nonglove_dict = {word: np.random.normal(0, 1, 100) for word in train_shared['wordCounter'] if word not in train_shared['word2vec']}\r\n",
        "    train_shared['word2vec'].update(nonglove_dict)\r\n",
        "    \r\n",
        "    val_shared = pd.read_pickle(val_shared_pth)\r\n",
        "    val_nonglove_dict = {word: np.random.normal(0, 1, 100) for word in val_shared['wordCounter'] if word not in val_shared['word2vec']}\r\n",
        "    val_shared['word2vec'].update(val_nonglove_dict)\r\n",
        "\r\n",
        "    test_shared = pd.read_pickle(test_shared_pth)\r\n",
        "    test_nonglove_dict = {word: np.random.normal(0, 1, 100) for word in test_shared['wordCounter'] if word not in test_shared['word2vec']}\r\n",
        "    test_shared['word2vec'].update(test_nonglove_dict)\r\n",
        "\r\n",
        "    train_data = MemexQA_new(data=pd.read_pickle(train_data_pth), shared=train_shared)\r\n",
        "    valid_data = MemexQA_new(data=pd.read_pickle(val_data_pth), shared=val_shared)\r\n",
        "    test_data = MemexQA_new(data=pd.read_pickle(test_data_pth), shared=test_shared)\r\n",
        "\r\n",
        "    train_loader_args = dict(shuffle=True, batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\r\n",
        "        else dict(shuffle=True, batch_size=BATCH_SIZE, collate_fn=train_collate)\r\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, **train_loader_args)\r\n",
        "\r\n",
        "    valid_loader_args = dict(shuffle=False, batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\r\n",
        "        else dict(shuffle=False, batch_size=BATCH_SIZE, collate_fn=train_collate)\r\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_data, **valid_loader_args)\r\n",
        "\r\n",
        "    test_loader_args = dict(shuffle=False, batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\r\n",
        "        else dict(shuffle=False, batch_size=BATCH_SIZE, collate_fn=train_collate)  # TODO: test_collate\r\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, **test_loader_args)\r\n",
        "    print(f\"Loading data took {time.time() - start:.1f} seconds\")\r\n",
        "    \r\n",
        "    # initialize model\r\n",
        "    device = torch.device(\"cuda\" if cuda else \"cpu\")\r\n",
        "    \r\n",
        "    #model = NewFusionModel(100, 3600,2537,128, 2, 2, device, 64, 64, 4, 3, 1)\r\n",
        "    #model = NewLSTMModel(100, 3600, 2537, 128, device)\r\n",
        "    #q_cs_input_size, desc_input_size, img_input_size, hidden_size, linear_size, k1, s1, k2, s2, batch_size, device\r\n",
        "    model = NewFusionModel(100, 3600,2537,128, 2, 2, device, 64, 64, 4, 3, 1)\r\n",
        "    model.to(device)\r\n",
        "\r\n",
        "    # setup optim and loss\r\n",
        "    criterion = nn.CrossEntropyLoss()\r\n",
        "    optimizer= optim.Adam(model.parameters(), lr = LR, weight_decay= WD)\r\n",
        "    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_STEPSIZE, gamma=LR_DECAY)\r\n",
        "\r\n",
        "    # training\r\n",
        "    for i in range(EPOCHS):\r\n",
        "        start = time.time()\r\n",
        "        print(\"Starting training......\")\r\n",
        "        model.train()\r\n",
        "        n_correct, n_total = 0, 0\r\n",
        "        batch_count = 0\r\n",
        "        t_loss = 0\r\n",
        "        q_totals = [0] * 5\r\n",
        "        q_corrects = [0] * 5\r\n",
        "        for j, (batch_data, batch_labels, qids, pids) in enumerate(train_loader):\r\n",
        "            if j == len(train_loader) - 1:\r\n",
        "                break\r\n",
        "            batch_labels = batch_labels.long().to(device)\r\n",
        "            optimizer.zero_grad()\r\n",
        "            output = model(batch_data)\r\n",
        "            loss = criterion(output, batch_labels)\r\n",
        "            t_loss += loss.item()\r\n",
        "            res = torch.argmax(output, 1)\r\n",
        "            res = res.to(device)\r\n",
        "            for i, qid in enumerate(qids):\r\n",
        "                if qid in qtype2qid[\"when\"]:\r\n",
        "                    q_totals[0] += 1\r\n",
        "                    if batch_labels[i] == res[i]:\r\n",
        "                        q_corrects[0] += 1\r\n",
        "                if qid in qtype2qid[\"what\"]:\r\n",
        "                    q_totals[1] += 1\r\n",
        "                    if batch_labels[i] == res[i]:\r\n",
        "                        q_corrects[1] += 1\r\n",
        "                if qid in qtype2qid[\"who\"]:\r\n",
        "                    q_totals[2] += 1\r\n",
        "                    if batch_labels[i] == res[i]:\r\n",
        "                        q_corrects[2] += 1\r\n",
        "                if qid in qtype2qid[\"where\"]:\r\n",
        "                    q_totals[3] += 1\r\n",
        "                    if batch_labels[i] == res[i]:\r\n",
        "                        q_corrects[3] += 1\r\n",
        "                if qid in qtype2qid[\"how\"]:\r\n",
        "                    q_totals[4] += 1\r\n",
        "                    if batch_labels[i] == res[i]:\r\n",
        "                        q_corrects[4] += 1\r\n",
        "            n_correct += (res == batch_labels).sum().item()\r\n",
        "            n_total += batch_labels.shape[0]\r\n",
        "            batch_count += 1\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            #if batch_count % 20 == 19:\r\n",
        "            #    print(f\"correct choice:{batch_labels[:3]} , predicted choice: {res[:3]}\")\r\n",
        "        train_acc = n_correct / n_total\r\n",
        "        train_loss = t_loss / batch_count\r\n",
        "        print(f\"TRAIN ===> Epoch {i}, took time {time.time()-start:.1f}s, train accu: {train_acc:.4f}, train loss: {train_loss:.6f}\")\r\n",
        "        train_acc_q_type = np.array(q_corrects) /np.array(q_totals)\r\n",
        "        for i in range(5):  # 5 types of questions\r\n",
        "            print(\"TRAIN ACC \", q_types[i], \": \", train_acc_q_type[i])\r\n",
        "        #scheduler.step()\r\n",
        "        \r\n",
        "        # validate and save model \r\n",
        "        print(\"Start validation......\")\r\n",
        "        start = time.time()\r\n",
        "        with torch.no_grad():\r\n",
        "            model.eval()            \r\n",
        "            valid_correct, loss, num_of_batches, num_of_val = 0, 0, 0, 0\r\n",
        "            # validation for classification\r\n",
        "            q_totals = [0] * 5\r\n",
        "            q_corrects = [0] * 5\r\n",
        "            for j, (vb_data, vb_label, qids, pids) in enumerate(valid_loader):\r\n",
        "                if j == len(valid_loader) - 1:\r\n",
        "                    break\r\n",
        "                vb_label = vb_label.long().to(device)\r\n",
        "                v_output = model(vb_data)\r\n",
        "                resm = torch.argmax(v_output, 1)\r\n",
        "                resm = resm.to(device)\r\n",
        "                for i, qid in enumerate(qids):\r\n",
        "                    if qid in qtype2qid[\"when\"]:\r\n",
        "                        q_totals[0] += 1\r\n",
        "                        if vb_label[i] == resm[i]:\r\n",
        "                            q_corrects[0] += 1\r\n",
        "                    if qid in qtype2qid[\"what\"]:\r\n",
        "                        q_totals[1] += 1\r\n",
        "                        if vb_label[i] == resm[i]:\r\n",
        "                            q_corrects[1] += 1\r\n",
        "                    if qid in qtype2qid[\"who\"]:\r\n",
        "                        q_totals[2] += 1\r\n",
        "                        if vb_label[i] == resm[i]:\r\n",
        "                            q_corrects[2] += 1\r\n",
        "                    if qid in qtype2qid[\"where\"]:\r\n",
        "                        q_totals[3] += 1\r\n",
        "                        if vb_label[i] == resm[i]:\r\n",
        "                            q_corrects[3] += 1\r\n",
        "                    if qid in qtype2qid[\"how\"]:\r\n",
        "                        q_totals[4] += 1\r\n",
        "                        if vb_label[i] == resm[i]:\r\n",
        "                            q_corrects[4] += 1\r\n",
        "                correct = (resm == vb_label).sum().item()\r\n",
        "                valid_correct += correct\r\n",
        "                loss += criterion(v_output, vb_label).item()\r\n",
        "                num_of_batches += 1\r\n",
        "                num_of_val += vb_label.shape[0]\r\n",
        "                #if num_of_batches % 20 == 19:\r\n",
        "                #    print(f\"correct choice:{vb_label[:3]} , predicted choice: {resm[:3]}\")\r\n",
        "            val_loss = loss / num_of_batches\r\n",
        "            val_accu = valid_correct / num_of_val\r\n",
        "        print(f\"VALID ===> Epoch {i}, took time {time.time()-start:.1f}s, valid accu: {val_accu:.4f}, valid loss: {val_loss:.6f}\")\r\n",
        "        train_acc_q_type = np.array(q_corrects) /np.array(q_totals)\r\n",
        "        for i in range(5):  # 5 types of questions\r\n",
        "            print(\"VALID ACC \", q_types[i], \": \", train_acc_q_type[i])\r\n",
        "        \r\n",
        "        snapshot_prefix = os.path.join(os.getcwd(), 'snapshot/')\r\n",
        "        if not os.path.exists(snapshot_prefix):\r\n",
        "            os.makedirs(snapshot_prefix)\r\n",
        "        torch.save({\r\n",
        "                    'model_state_dict': model.state_dict(),\r\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\r\n",
        "                    #'scheduler_state_dict' : scheduler.state_dict(),\r\n",
        "        }, snapshot_prefix + \"Model_\"+str(i))\r\n",
        "    \r\n",
        "    # testing\r\n",
        "    if not isTrain:\r\n",
        "        print(\"Start testing......\")\r\n",
        "        start = time.time()\r\n",
        "        model.eval()\r\n",
        "        with torch.no_grad(), open('test_predictions.csv', 'w') as f:\r\n",
        "            writer = csv.writer(f, delimiter=',')\r\n",
        "            writer.writerow([\"predict\",\"actual\"])\r\n",
        "            for (tbatch_data, tbatch_data_labels) in test_loader:\r\n",
        "                test_out = model(tbatch_data)\r\n",
        "                predict = torch.argmax(test_out, axis=1)\r\n",
        "                correct = (predict == tbatch_data_labels).sum().item()\r\n",
        "                for (pred, actual) in zip(predict, correct):\r\n",
        "                    writer.writerow([pred, actual])\r\n",
        "        print(f\"Testing took {time.time()-start:.1f}s\")\r\n",
        "    print(\"Finished\")\r\n",
        "                \r\n",
        "                                                    \r\n",
        "    \r\n",
        "main('prepro_v1.1/train_data.p', 'prepro_v1.1/train_shared.p', \r\n",
        "     'prepro_v1.1/val_data.p', 'prepro_v1.1/val_shared.p', \r\n",
        "     'prepro_v1.1/test_data.p', 'prepro_v1.1/test_shared.p',\r\n",
        "     isTrain = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data......\n",
            "Loading data took 13.8 seconds\n",
            "Starting training......\n",
            "TRAIN ===> Epoch 63, took time 82.9s, train accu: 0.4071, train loss: 1.249992\n",
            "TRAIN ACC  when :  0.3073192239858907\n",
            "TRAIN ACC  what :  0.4613916363005379\n",
            "TRAIN ACC  who :  0.23581560283687944\n",
            "TRAIN ACC  where :  0.24806576402321084\n",
            "TRAIN ACC  how :  0.7062464828362408\n",
            "Start validation......\n",
            "VALID ===> Epoch 63, took time 21.0s, valid accu: 0.4422, valid loss: 1.213766\n",
            "VALID ACC  when :  0.36212624584717606\n",
            "VALID ACC  what :  0.5031667839549613\n",
            "VALID ACC  who :  0.2887700534759358\n",
            "VALID ACC  where :  0.26582278481012656\n",
            "VALID ACC  how :  0.7058823529411765\n",
            "Starting training......\n",
            "TRAIN ===> Epoch 63, took time 82.8s, train accu: 0.4639, train loss: 1.166584\n",
            "TRAIN ACC  when :  0.3237885462555066\n",
            "TRAIN ACC  what :  0.5576756287944492\n",
            "TRAIN ACC  who :  0.2829297105729474\n",
            "TRAIN ACC  where :  0.28267182962245885\n",
            "TRAIN ACC  how :  0.7220969560315671\n",
            "Start validation......\n",
            "VALID ===> Epoch 63, took time 21.1s, valid accu: 0.4493, valid loss: 1.187841\n",
            "VALID ACC  when :  0.3554817275747508\n",
            "VALID ACC  what :  0.5573539760731879\n",
            "VALID ACC  who :  0.20855614973262032\n",
            "VALID ACC  where :  0.22965641952983726\n",
            "VALID ACC  how :  0.7081447963800905\n",
            "Starting training......\n",
            "TRAIN ===> Epoch 63, took time 82.7s, train accu: 0.4893, train loss: 1.119196\n",
            "TRAIN ACC  when :  0.32130281690140844\n",
            "TRAIN ACC  what :  0.6072047107724281\n",
            "TRAIN ACC  who :  0.2820816085156712\n",
            "TRAIN ACC  where :  0.3055286129970902\n",
            "TRAIN ACC  how :  0.7326172979084229\n",
            "Start validation......\n",
            "VALID ===> Epoch 63, took time 20.9s, valid accu: 0.4522, valid loss: 1.193450\n",
            "VALID ACC  when :  0.3637873754152824\n",
            "VALID ACC  what :  0.5622800844475722\n",
            "VALID ACC  who :  0.22192513368983957\n",
            "VALID ACC  where :  0.22423146473779385\n",
            "VALID ACC  how :  0.6990950226244343\n",
            "Starting training......\n",
            "TRAIN ===> Epoch 63, took time 82.5s, train accu: 0.5119, train loss: 1.082378\n",
            "TRAIN ACC  when :  0.3215547703180212\n",
            "TRAIN ACC  what :  0.6404922863581209\n",
            "TRAIN ACC  who :  0.325044404973357\n",
            "TRAIN ACC  where :  0.32220609579100146\n",
            "TRAIN ACC  how :  0.7352445193929174\n",
            "Start validation......\n",
            "VALID ===> Epoch 63, took time 20.9s, valid accu: 0.4631, valid loss: 1.191468\n",
            "VALID ACC  when :  0.33554817275747506\n",
            "VALID ACC  what :  0.5805770584095707\n",
            "VALID ACC  who :  0.25133689839572193\n",
            "VALID ACC  where :  0.24954792043399637\n",
            "VALID ACC  how :  0.7058823529411765\n",
            "Starting training......\n",
            "TRAIN ===> Epoch 63, took time 82.5s, train accu: 0.5235, train loss: 1.056639\n",
            "TRAIN ACC  when :  0.3086092715231788\n",
            "TRAIN ACC  what :  0.668861549124935\n",
            "TRAIN ACC  who :  0.3287914691943128\n",
            "TRAIN ACC  where :  0.33010648596321396\n",
            "TRAIN ACC  how :  0.7350956130483689\n",
            "Start validation......\n",
            "VALID ===> Epoch 63, took time 21.1s, valid accu: 0.4561, valid loss: 1.208354\n",
            "VALID ACC  when :  0.33222591362126247\n",
            "VALID ACC  what :  0.5756509500351865\n",
            "VALID ACC  who :  0.22994652406417113\n",
            "VALID ACC  where :  0.23508137432188064\n",
            "VALID ACC  how :  0.7081447963800905\n",
            "Starting training......\n",
            "TRAIN ===> Epoch 63, took time 82.6s, train accu: 0.5403, train loss: 1.035549\n",
            "TRAIN ACC  when :  0.3299469964664311\n",
            "TRAIN ACC  what :  0.6904555690282349\n",
            "TRAIN ACC  who :  0.3514792899408284\n",
            "TRAIN ACC  where :  0.3333333333333333\n",
            "TRAIN ACC  how :  0.7412626832018039\n",
            "Start validation......\n",
            "VALID ===> Epoch 63, took time 20.8s, valid accu: 0.4522, valid loss: 1.227885\n",
            "VALID ACC  when :  0.34551495016611294\n",
            "VALID ACC  what :  0.5594651653764954\n",
            "VALID ACC  who :  0.23796791443850268\n",
            "VALID ACC  where :  0.23508137432188064\n",
            "VALID ACC  how :  0.7058823529411765\n",
            "Starting training......\n",
            "TRAIN ===> Epoch 63, took time 82.4s, train accu: 0.5455, train loss: 1.013801\n",
            "TRAIN ACC  when :  0.3164612676056338\n",
            "TRAIN ACC  what :  0.7023933402705516\n",
            "TRAIN ACC  who :  0.34772324068598465\n",
            "TRAIN ACC  where :  0.3543383422200679\n",
            "TRAIN ACC  how :  0.7398648648648649\n",
            "Start validation......\n",
            "VALID ===> Epoch 63, took time 20.9s, valid accu: 0.4626, valid loss: 1.213261\n",
            "VALID ACC  when :  0.33056478405315615\n",
            "VALID ACC  what :  0.5960591133004927\n",
            "VALID ACC  who :  0.22994652406417113\n",
            "VALID ACC  where :  0.24050632911392406\n",
            "VALID ACC  how :  0.6877828054298643\n",
            "Starting training......\n",
            "TRAIN ===> Epoch 63, took time 82.8s, train accu: 0.5556, train loss: 0.995177\n",
            "TRAIN ACC  when :  0.3305346884666372\n",
            "TRAIN ACC  what :  0.7153499653499653\n",
            "TRAIN ACC  who :  0.3617525162818236\n",
            "TRAIN ACC  where :  0.3528557599225557\n",
            "TRAIN ACC  how :  0.7429696287964005\n",
            "Start validation......\n",
            "VALID ===> Epoch 63, took time 20.8s, valid accu: 0.4578, valid loss: 1.251455\n",
            "VALID ACC  when :  0.3106312292358804\n",
            "VALID ACC  what :  0.5791695988740324\n",
            "VALID ACC  who :  0.23529411764705882\n",
            "VALID ACC  where :  0.25316455696202533\n",
            "VALID ACC  how :  0.7126696832579186\n",
            "Starting training......\n",
            "TRAIN ===> Epoch 63, took time 82.7s, train accu: 0.5576, train loss: 0.985073\n",
            "TRAIN ACC  when :  0.3309828118113706\n",
            "TRAIN ACC  what :  0.7271780631725095\n",
            "TRAIN ACC  who :  0.3473124630832841\n",
            "TRAIN ACC  where :  0.34704743465634075\n",
            "TRAIN ACC  how :  0.7418447694038245\n",
            "Start validation......\n",
            "VALID ===> Epoch 63, took time 20.9s, valid accu: 0.4673, valid loss: 1.252615\n",
            "VALID ACC  when :  0.32558139534883723\n",
            "VALID ACC  what :  0.6108374384236454\n",
            "VALID ACC  who :  0.21657754010695188\n",
            "VALID ACC  where :  0.22423146473779385\n",
            "VALID ACC  how :  0.7149321266968326\n",
            "Starting training......\n",
            "TRAIN ===> Epoch 63, took time 82.3s, train accu: 0.5631, train loss: 0.971782\n",
            "TRAIN ACC  when :  0.32422907488986785\n",
            "TRAIN ACC  what :  0.7317369425646365\n",
            "TRAIN ACC  who :  0.367612293144208\n",
            "TRAIN ACC  where :  0.3612590799031477\n",
            "TRAIN ACC  how :  0.7418447694038245\n",
            "Start validation......\n",
            "VALID ===> Epoch 63, took time 20.7s, valid accu: 0.4590, valid loss: 1.281124\n",
            "VALID ACC  when :  0.3388704318936877\n",
            "VALID ACC  what :  0.5946516537649542\n",
            "VALID ACC  who :  0.18449197860962566\n",
            "VALID ACC  where :  0.22423146473779385\n",
            "VALID ACC  how :  0.7126696832579186\n",
            "Finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7v5_ryrFTPd"
      },
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import argparse\n",
        "import csv\n",
        "\n",
        "# hyperparams\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# optimizer-related\n",
        "MOMENTUM = 1e-2\n",
        "LR = 1e-2\n",
        "LR_STEPSIZE = 3\n",
        "LR_DECAY = 0.85\n",
        "WD = 5e-6\n",
        "\n",
        "\n",
        "\n",
        "def main(train_data_pth, train_shared_pth, val_data_pth, val_shared_pth, test_data_pth, test_shared_pth, isTrain):\n",
        "    cuda = torch.cuda.is_available()\n",
        "    num_workers = 8 if cuda else 0\n",
        "    print(\"Loading data......\")\n",
        "    start = time.time()\n",
        "    # load data\n",
        "    # train_shared = pd.read_pickle('prepro_v1.1/train_shared.p')\n",
        "    train_shared = pd.read_pickle(train_shared_pth)\n",
        "    # random initial embedding matrix for new words\n",
        "    nonglove_dict = {word: np.random.normal(0, 1, 100) for word in train_shared['wordCounter'] if word not in train_shared['word2vec']}\n",
        "    train_shared['word2vec'].update(nonglove_dict)\n",
        "    \n",
        "    val_shared = pd.read_pickle(val_shared_pth)\n",
        "    val_nonglove_dict = {word: np.random.normal(0, 1, 100) for word in val_shared['wordCounter'] if word not in val_shared['word2vec']}\n",
        "    val_shared['word2vec'].update(val_nonglove_dict)\n",
        "\n",
        "    test_shared = pd.read_pickle(test_shared_pth)\n",
        "    test_nonglove_dict = {word: np.random.normal(0, 1, 100) for word in test_shared['wordCounter'] if word not in test_shared['word2vec']}\n",
        "    test_shared['word2vec'].update(test_nonglove_dict)\n",
        "\n",
        "    # train_data = MemexQA_new(data=pd.read_pickle('prepro_v1.1/train_data.p'), shared=train_shared, info=None)\n",
        "    # valid_data = MemexQA_new(data=pd.read_pickle('prepro_v1.1/val_data.p'), shared=val_shared, info=None)\n",
        "    # test_data = MemexQA_new(data=pd.read_pickle('prepro_v1.1/test_data.p'), shared=test_shared, info=None)\n",
        "    train_data = MemexQA_new(data=pd.read_pickle(train_data_pth), shared=train_shared)\n",
        "    valid_data = MemexQA_new(data=pd.read_pickle(val_data_pth), shared=val_shared)\n",
        "    test_data = MemexQA_new(data=pd.read_pickle(test_data_pth), shared=test_shared)\n",
        "\n",
        "    # random initial embedding matrix for new words\n",
        "    # config.emb_mat = np.array([idx2vec_dict[idx] if idx2vec_dict.has_key(idx) \n",
        "    # else np.random.multivariate_normal(np.zeros(config.word_emb_size), np.eye(config.word_emb_size)) \n",
        "    # for idx in xrange(config.word_vocab_size)],dtype=\"float32\") \n",
        "\n",
        "    train_loader_args = dict(shuffle=True, batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\n",
        "        else dict(shuffle=True, batch_size=BATCH_SIZE, collate_fn=train_collate)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, **train_loader_args)\n",
        "\n",
        "    valid_loader_args = dict(batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\n",
        "        else dict(shuffle=False, batch_size=BATCH_SIZE, collate_fn=train_collate)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_data, **valid_loader_args)\n",
        "\n",
        "    test_loader_args = dict(batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\n",
        "        else dict(shuffle=False, batch_size=BATCH_SIZE, collate_fn=train_collate)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, **test_loader_args)\n",
        "    print(f\"Loading data took {time.time() - start:.1f} seconds\")\n",
        "    \n",
        "    # initialize model\n",
        "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "    #model = SimpleLSTMModel(100, 64, hyperparams['batch_size'], 2, device)\n",
        "    # input_size, hidden_size, batch_size, num_layers, device, q_linear_size, img_linear_size, multimodal_out, kernel, stride\n",
        "    \n",
        "    model = NewFusionModel(100, 3100,2537,128, 2, 2, device, 64, 64, 4, 3, 1)\n",
        "    model.to(device)\n",
        "\n",
        "    # setup optim and loss\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    #optimizer= optim.SGD(model.parameters(), momentum=hyperparams['momentum'], lr = hyperparams['lr'], weight_decay= hyperparams['weight_decay'])\n",
        "    optimizer= optim.Adam(model.parameters(), lr = LR, weight_decay= WD)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_STEPSIZE, gamma=LR_DECAY)\n",
        "\n",
        "    # training\n",
        "    print(\"Starting training......\")\n",
        "    for i in range(EPOCHS):\n",
        "        start = time.time()\n",
        "        model.train()\n",
        "        n_correct,n_total = 0, 0\n",
        "        batch_count = 0\n",
        "        t_loss = 0\n",
        "        for j, (batch_data, batch_labels) in enumerate(train_loader):\n",
        "            batch_labels = batch_labels.long().to(device)\n",
        "            if j == len(train_loader) - 1:\n",
        "                break\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch_data)\n",
        "            loss = criterion(output, batch_labels)\n",
        "            t_loss += loss.item()\n",
        "            res = torch.argmax(output, 1)\n",
        "            res = res.to(device)\n",
        "            n_correct += (res == batch_labels).sum().item()\n",
        "            n_total += batch_labels.shape[0]\n",
        "            batch_count += 1\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if batch_count % 20 == 19:\n",
        "                print(f\"correct choice:{batch_labels[:3]} , predicted choice: {res[:3]}\")\n",
        "        train_acc = n_correct / n_total\n",
        "        train_loss = t_loss / batch_count\n",
        "        print(f\"TRAIN ===> Epoch {i}, took time {time.time()-start:.1f}s, train accu: {train_acc:.4f}, train loss: {train_loss:.6f}\")\n",
        "        scheduler.step()\n",
        "        \n",
        "        # validate and save model \n",
        "        print(\"Start validation......\")\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            model.eval()            \n",
        "            valid_correct, loss, num_of_batches, num_of_val = 0, 0, 0, 0\n",
        "            # validation for classification\n",
        "            for (vb_data, vb_label) in valid_loader:\n",
        "                vb_label = vb_label.long().to(device)\n",
        "                v_output = model(vb_data)\n",
        "                resm = torch.argmax(v_output, axis=1)\n",
        "                resm = resm.to(device)\n",
        "                correct = (resm == vb_label).sum().item()\n",
        "                valid_correct += correct\n",
        "                loss += criterion(v_output, vb_label).item()\n",
        "                num_of_batches += 1\n",
        "                num_of_val += vb_label.shape[0]\n",
        "                if num_of_batches % 20 == 19:\n",
        "                    print(f\"correct choice:{vb_label[:3]} , predicted choice: {resm[:3]}\")\n",
        "            val_loss = loss / num_of_batches\n",
        "            val_accu = valid_correct / num_of_val\n",
        "        print(f\"VALID ===> Epoch {i}, took time {time.time()-start:.1f}s, valid accu: {val_accu:.4f}, valid loss: {val_loss:.6f}\")\n",
        "        \n",
        "        snapshot_prefix = os.path.join(os.getcwd(), 'snapshot/')\n",
        "        if not os.path.exists(snapshot_prefix):\n",
        "            os.makedirs(snapshot_prefix)\n",
        "        torch.save({\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict' : scheduler.state_dict(),\n",
        "        }, snapshot_prefix + \"Model_\"+str(i))\n",
        "    \n",
        "    # testing\n",
        "    if not isTrain:\n",
        "        print(\"Start testing......\")\n",
        "        start = time.time()\n",
        "        model.eval()\n",
        "        with torch.no_grad(), open('test_predictions.csv', 'w') as f:\n",
        "            writer = csv.writer(f, delimiter=',')\n",
        "            writer.writerow([\"predict\",\"actual\"])\n",
        "            for (tbatch_data, tbatch_data_labels) in test_loader:\n",
        "                test_out = model(tbatch_data)\n",
        "                predict = torch.argmax(test_out, axis=1)\n",
        "                correct = (predict == tbatch_data_labels).sum().item()\n",
        "                for (pred, actual) in zip(predict, correct):\n",
        "                    writer.writerow([pred, actual])\n",
        "        print(f\"Testing took {time.time()-start:.1f}s\")\n",
        "    print(\"Finished\")\n",
        "                \n",
        "                                                    \n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    main('prepro_v1.1/train_data.p',\n",
        "        'prepro_v1.1/train_shared.p',\n",
        "        'prepro_v1.1/val_data.p',\n",
        "        'prepro_v1.1/val_shared.p',\n",
        "        'prepro_v1.1/test_data.p',\n",
        "        'prepro_v1.1/test_shared.p',\n",
        "        isTrain = True)\n",
        "    # parser = argparse.ArgumentParser(description='Get the train-val-test dataset files')\n",
        "    \n",
        "    # parser.add_argument(\"-td\" , \"--train_data_pth\", help=\"Enter train data path\", type=str)\n",
        "    # parser.add_argument(\"-tds\", \"--train_shared_pth\", help=\"Enter train_shared data path\", type=str)\n",
        "    # parser.add_argument(\"-vd\", \"--val_data_pth\", help=\"Enter val data path\", type=str)\n",
        "    # parser.add_argument(\"-vds\", \"--val_shared_pth\", help=\"Enter val_shared data path\", type=str)\n",
        "    # parser.add_argument(\"-test\", \"--test_data_pth\", help=\"Enter test data path\", type=str)\n",
        "    # parser.add_argument(\"-test_shared\", \"--test_shared_pth\", help=\"Enter test_shared data path\", type=str)\n",
        "    # # parser.add_argument(\"-album\", \"--album_data_pth\", help=\"Enter album_json data path\", type=str)\n",
        "    # parser.add_argument(\"isTrain\", help=\"Set True if model is training\", type=bool)\n",
        "    \n",
        "    # args = parser.parse_args()\n",
        "    \n",
        "    # main(args.train_data_pth,\n",
        "    #     args.train_shared_pth,\n",
        "    #     args.val_data_pth,\n",
        "    #     args.val_shared_pth,\n",
        "    #     args.test_data_pth,\n",
        "    #     args.test_shared_pth,\n",
        "    #     isTrain = args.isTrain)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}