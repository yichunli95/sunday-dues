{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-12 15:54:23--  https://memexqa.cs.cmu.edu/fvta_model_zoo/prepro_v1.1.tgz\n",
      "Resolving memexqa.cs.cmu.edu (memexqa.cs.cmu.edu)... 128.2.220.9\n",
      "Connecting to memexqa.cs.cmu.edu (memexqa.cs.cmu.edu)|128.2.220.9|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 212511000 (203M) [application/x-gzip]\n",
      "Saving to: ‘prepro_v1.1.tgz’\n",
      "\n",
      "prepro_v1.1.tgz     100%[===================>] 202.67M  45.4MB/s    in 9.4s    \n",
      "\n",
      "2020-12-12 15:54:32 (21.5 MB/s) - ‘prepro_v1.1.tgz’ saved [212511000/212511000]\n",
      "\n",
      "prepro_v1.1/\n",
      "prepro_v1.1/test_data.p\n",
      "prepro_v1.1/train_shared.p\n",
      "prepro_v1.1/test_shared.p\n",
      "prepro_v1.1/train_data.p\n",
      "prepro_v1.1/val_data.p\n",
      "prepro_v1.1/val_shared.p\n"
     ]
    }
   ],
   "source": [
    "!wget https://memexqa.cs.cmu.edu/fvta_model_zoo/prepro_v1.1.tgz\n",
    "!gunzip prepro_v1.1.tgz\n",
    "!tar -xvf prepro_v1.1.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import *\n",
    "import torch.nn.functional as F\n",
    "# import new_dataset_checked_by_hongyuan\n",
    "# import pandas as pd\n",
    "\n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self, q_cs_input_size, desc_input_size, img_input_size, hidden_size, batch_size,\n",
    "                num_layers, device, img_linear_size,num_choices = 4, rnn_type = 'bilstm'):\n",
    "        super(AttentionModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.q_cs_input_size = q_cs_input_size   #input_size qvec and cs_vec\n",
    "        self.desc_input_size = desc_input_size\n",
    "        self.img_input_size = img_input_size\n",
    "        self.img_linear_size = img_linear_size # s2\n",
    "        self.num_directions = 2 if rnn_type == 'bilstm' else 1\n",
    "        self.num_layers = num_layers\n",
    "        self.cos = nn.CosineSimilarity(dim=2, eps=1e-6)\n",
    "        self.softmax1 = nn.Softmax(dim = 1)\n",
    "        self.softmax2 = nn.Softmax(dim = 2)\n",
    "\n",
    "        if (rnn_type == 'bilstm'):\n",
    "            self.rnn_q = nn.LSTM(q_cs_input_size, hidden_size, self.num_layers, batch_first = False, bidirectional = True) # questions\n",
    "            self.rnn_c = nn.LSTM(q_cs_input_size, hidden_size, self.num_layers, batch_first = False, bidirectional = True) # choices\n",
    "            self.rnn_desc = nn.LSTM(desc_input_size, hidden_size, self.num_layers, batch_first = False, bidirectional = True) # photo titles\n",
    "            self.rnn_ps = nn.LSTM(img_input_size, hidden_size, self.num_layers, batch_first = False, bidirectional = True) # image features\n",
    "        else:\n",
    "            self.rnn_q = nn.LSTM(q_cs_input_size, hidden_size, self.num_layers, batch_first = False, bidirectional = False)\n",
    "            self.rnn_c = nn.LSTM(q_cs_input_size, hidden_size, self.num_layers, batch_first = False, bidirectional = False)\n",
    "            self.rnn_desc = nn.LSTM(desc_input_size, hidden_size, self.num_layers, batch_first = False, bidirectional = False)\n",
    "            self.rnn_ps = nn.LSTM(img_input_size, hidden_size, self.num_layers, batch_first = False, bidirectional = False)\n",
    "\n",
    "        self.vis_text = nn.Linear(2*hidden_size, 2*hidden_size)\n",
    "      \n",
    "        self.tanh1 = nn.Tanh()\n",
    "        self.CH_linear = nn.Linear(2*hidden_size, 2*hidden_size)\n",
    "        self.img_linear = nn.Linear(hidden_size, self.img_linear_size) \n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.tanh3 = nn.Tanh()\n",
    "\n",
    "        #input: B X num_choices X (5 * 2 * hidden_size), output: B x num_choices X 1\n",
    "        self.last_softmax = nn.Linear(5 * 2 * hidden_size , 1) \n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # X is a list of dictionaries: 'q_vec', 'cs_vec', 'desc_vec', 'img_feats'\n",
    "        # BATCH_SIZE = len(X)\n",
    "        # B, T, F\n",
    "        # q_vec -> B, T, 100 (glove embedding)\n",
    "        # cs_vec -> B, 4, Y_THRES, 100\n",
    "        # desc_vec(prev. pt) -> B, all_photo_titles_albums * 40, 100\n",
    "        # img_feats -> B, all_photo_titles_albums, 2537\n",
    "\n",
    "        q_vec = X['q_vec']\n",
    "        cs_vec = X['cs_vec']\n",
    "        desc_vec = X['desc_vec']\n",
    "        img_feats = X['img_feats']\n",
    "\n",
    "        packed_q_vec = pack_padded_sequence(q_vec, X['q_len'], batch_first=False, enforce_sorted = False).to(self.device)\n",
    "        packed_c0_vec = pack_padded_sequence(cs_vec[:, :, 0, :], X['cs0_lens'], batch_first = False, enforce_sorted = False).to(self.device)\n",
    "        packed_c1_vec = pack_padded_sequence(cs_vec[:, :, 1, :], X['cs1_lens'], batch_first = False, enforce_sorted = False).to(self.device)\n",
    "        packed_c2_vec = pack_padded_sequence(cs_vec[:, :, 2, :], X['cs2_lens'], batch_first = False, enforce_sorted = False).to(self.device)\n",
    "        packed_c3_vec = pack_padded_sequence(cs_vec[:, :, 3, :], X['cs3_lens'], batch_first = False, enforce_sorted = False).to(self.device)\n",
    "        packed_pt_vec = pack_padded_sequence(desc_vec, X['desc_len'], batch_first = False, enforce_sorted = False).to(self.device)\n",
    "        packed_img_vec = pack_padded_sequence(img_feats, X['img_len'], batch_first = False, enforce_sorted = False).to(self.device)\n",
    "        q_out,_   = self.rnn_q(packed_q_vec)  # M X B X 2d\n",
    "        c0_out,_  = self.rnn_c(packed_c0_vec) # T X B X 2d\n",
    "        c1_out,_  = self.rnn_c(packed_c1_vec) # T X B X 2d\n",
    "        c2_out,_  = self.rnn_c(packed_c2_vec) # T X B X 2d\n",
    "        c3_out,_  = self.rnn_c(packed_c3_vec) # T X B X 2d\n",
    "        text_out,_ = self.rnn_desc(packed_pt_vec) # T X B X 2d\n",
    "        vis_out,_  = self.rnn_ps(packed_img_vec) # T X B X 2d\n",
    "\n",
    "        # print(\"vis_out: \", vis_out.data.shape, text_out.data.shape)\n",
    "\n",
    "\n",
    "        q_out, q_lens_unpacked = pad_packed_sequence(q_out, batch_first=False)  # M X B X 2d\n",
    "        q_out = q_out.permute(1,0,2) # B X M X 2d\n",
    "        c0_out, c0_lens_unpacked = pad_packed_sequence(c0_out, batch_first=False)  # T X B X 2d\n",
    "        c0_out = c0_out[-1,:,:].unsqueeze(0).permute(1,0,2) # B X 1 X 2d\n",
    "        c1_out, c1_lens_unpacked = pad_packed_sequence(c1_out, batch_first=False)  # T X B X 2d\n",
    "        c1_out = c1_out[-1,:,:].unsqueeze(0).permute(1,0,2) # B X 1 X 2d\n",
    "        c2_out, c2_lens_unpacked = pad_packed_sequence(c2_out, batch_first=False)    # T X B X 2d\n",
    "        c2_out = c2_out[-1,:,:].unsqueeze(0).permute(1,0,2) # B X 1 X 2d\n",
    "        c3_out, c3_lens_unpacked = pad_packed_sequence(c3_out, batch_first=False)   # T X B X 2d\n",
    "        c3_out = c3_out[-1,:,:].unsqueeze(0).permute(1,0,2) # B X 1 X 2d\n",
    "        txt_out, text_out_lens = pad_packed_sequence(text_out, batch_first=False) # T X B X 2d\n",
    "        txt_out = txt_out.permute(1,0,2) # B X T X 2d\n",
    "       \n",
    "\n",
    "        vis_out, vis_out_lens = pad_packed_sequence(vis_out, batch_first=False) # T X B X 2d\n",
    "        vis_out = vis_out.permute(1,0,2) # B X T X 2d    \n",
    "        \n",
    "        # print(\"vis_out: \", vis_out.shape, txt_out.shape)\n",
    "\n",
    "        # print(\"qout: \", q_out.shape)\n",
    "        # correlation b/w txt_out and vis_out : B X T X 2d -> linear_layer \n",
    "        vis = self.vis_text(vis_out)      # B X T X 2d\n",
    "        vis_out = torch.unsqueeze(vis_out, 3) \n",
    "        # print(\"vis: \", vis.shape)\n",
    "        text = self.vis_text(txt_out)      # B X T X 2d\n",
    "        txt_out = torch.unsqueeze(txt_out, 3)\n",
    "       \n",
    "        H = torch.cat([vis_out, txt_out], dim = 3)  # B * T * 2d * 2\n",
    "        # print(\"H: \", H.shape)  \n",
    "        C = self.tanh1(vis @ text.permute(0,2,1))  # B X T X T\n",
    "        C_repeat = C.unsqueeze(3).repeat(1,1,1,2) # B X T x T X 2\n",
    "        # print(\"C: \", C_repeat.shape)\n",
    "        \n",
    "        \n",
    "        H_perm = H.permute(0, 3, 2, 1) # B x 2 x 2d x T\n",
    "        # print(\"Hperm shape:\", H_perm.shape)\n",
    "        C_perm = C_repeat.permute(0,3,1,2) # B x 2 x T x T\n",
    "        # print(\"Cperm shape:\", C_perm.shape)\n",
    "        F = torch.matmul(H_perm, C_perm) # B x 2 X 2d x T\n",
    "       \n",
    "        F = F.permute(0,3,1,2) #F:  B * T * 2 * 2d\n",
    "        F = self.CH_linear(F) \n",
    "        F = self.tanh2(F)\n",
    "        # print(\"F shape:\", F.shape) \n",
    "\n",
    "        E = torch.cat([c0_out,c1_out, c2_out, c3_out], dim = 1)  #B x 4 X 2d\n",
    "        # print(\"E shape:\", E.shape) \n",
    "        Q = q_out  #   Q: B x M x 2d\n",
    "        # print(\"Q shape:\", Q.shape) \n",
    "        \n",
    "        bmm1 = Q.bmm(F.permute(0,3,1,2)[:,:,:,0]).unsqueeze(3)\n",
    "        bmm2 = Q.bmm(F.permute(0,3,1,2)[:,:,:,1]).unsqueeze(3)\n",
    "        S = torch.cat([bmm1, bmm2], dim = 3) #S:  B x M x T x 2\n",
    "        # print(\"S shape:\", S.shape) \n",
    "        S = self.tanh3(S)\n",
    "        max1 = torch.max(S, dim = 1)[0]\n",
    "        # print(\"max1 shape: \", max1.shape)\n",
    "        max2 = torch.max(max1, dim = 1)[0]\n",
    "        # print(\"max2 shape: \", max2.shape)\n",
    "        max3 = torch.max(S, dim = 3)[0]\n",
    "        # print(\"max3 shape: \", max3.shape)\n",
    "        max4 = torch.max(max3, dim = 2)[0]\n",
    "        # print(\"max4 shape: \", max4.shape)\n",
    "\n",
    "        '''\n",
    "        max1 shape:  torch.Size([3, 69, 2])\n",
    "        max2 shape:  torch.Size([3, 2])\n",
    "        max4 shape:  torch.Size([3, 9, 2])\n",
    "        max3 shape:  torch.Size([3, 9])\n",
    "                '''\n",
    "        A = self.softmax1(max1).to(self.device)  # B X T x 2\n",
    "        B = self.softmax1(max2).to(self.device)   # B X 2\n",
    "        D = self.softmax1(max4).to(self.device)  # B x M\n",
    "\n",
    "        h_tilda = torch.zeros((F.shape[0], F.shape[-1])).float().to(self.device) # B x 2d\n",
    "        q_tilda = torch.zeros((F.shape[0], F.shape[-1])).float().to(self.device) # B x 2d\n",
    "       \n",
    "        for k in range(2):\n",
    "            temp = torch.zeros((F.shape[0], F.shape[-1])).float().to(self.device)\n",
    "            for t in range(A.shape[1]):\n",
    "                # print(\"A shape: \", A[:,t, k].unsqueeze(1).repeat(1, F.shape[-1]).shape, \" , F[:, t, k, :] shape: \",  F[:, t, k, :].shape )\n",
    "                temp += A[:,t, k].unsqueeze(1).repeat(1, F.shape[-1]) *  F[:, t, k, :]# B x 2d\n",
    "                # print(\"temp shape: \", temp.shape)\n",
    "                # temp += A[:,t, k].unsqueeze(1).repeat(1, F.shape[0]) * F[:, t, k, :]\n",
    "            h_tilda +=  (B[:, k].unsqueeze(1).repeat(1, F.shape[-1]) * temp)\n",
    "            # print(\"htilda shape: \", h_tilda.shape)\n",
    "            # h_tilda += B[:, k] * temp    \n",
    "        \n",
    "        H_tilda = h_tilda.unsqueeze(1).repeat(1, 4, 1)  # B x 4 X 2d\n",
    "        # print(\"htilda: \", H_tilda.shape)\n",
    "\n",
    "        for m in range(D.shape[1]):\n",
    "            # print(\"D shape: \",D[:, m].unsqueeze(1).repeat(1, Q.shape[-1]).shape, \" Q shape: \", Q[:, m, :].shape )\n",
    "            q_tilda += D[:, m].unsqueeze(1).repeat(1, Q.shape[-1]) * Q[:, m, :]\n",
    "            # print(\"qtilda: \", q_tilda.shape)\n",
    "\n",
    "        Q_tilda = q_tilda.unsqueeze(1).repeat(1, 4, 1)  # B x 4 X 2d\n",
    "        # print(\"qtilda: \", Q_tilda.shape)\n",
    "        concat = torch.cat([Q_tilda, H_tilda, E, Q_tilda * E, H_tilda * E], dim = -1)\n",
    "\n",
    "        output = self.last_softmax(concat)\n",
    "\n",
    "        output = output.squeeze(2)\n",
    "        out_softmax = self.softmax1(output)\n",
    "        return out_softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import *\n",
    "from torch.utils.data import Dataset\n",
    "import itertools\n",
    "\n",
    "train_data = pd.read_pickle('prepro_v1.1/train_data.p')\n",
    "train_shared = pd.read_pickle('prepro_v1.1/train_shared.p')\n",
    "val_data = pd.read_pickle('prepro_v1.1/val_data.p')\n",
    "\n",
    "q_types = [\"when\", \"what\", \"who\", \"where\", \"how\"]\n",
    "qtype2qid = {}\n",
    "qtype2qid[\"when\"] = []\n",
    "qtype2qid[\"what\"] = []\n",
    "qtype2qid[\"who\"] = []\n",
    "qtype2qid[\"where\"] = []\n",
    "qtype2qid[\"how\"] = []\n",
    "\n",
    "for i, qid in enumerate(train_data['qid']):\n",
    "    if train_data['q'][i][0].lower() == \"when\":\n",
    "        qtype2qid[\"when\"].append(qid)\n",
    "    elif train_data['q'][i][0].lower() == \"what\":\n",
    "        qtype2qid[\"what\"].append(qid)\n",
    "    elif train_data['q'][i][0].lower() == \"who\":\n",
    "        qtype2qid[\"who\"].append(qid)\n",
    "    elif train_data['q'][i][0].lower() == \"where\":\n",
    "        qtype2qid[\"where\"].append(qid)\n",
    "    elif train_data['q'][i][0].lower() == \"how\":\n",
    "        qtype2qid[\"how\"].append(qid)\n",
    "\n",
    "for i, qid in enumerate(val_data['qid']):\n",
    "    if val_data['q'][i][0].lower() == \"when\":\n",
    "        qtype2qid[\"when\"].append(qid)\n",
    "    elif val_data['q'][i][0].lower() == \"what\":\n",
    "        qtype2qid[\"what\"].append(qid)\n",
    "    elif val_data['q'][i][0].lower() == \"who\":\n",
    "        qtype2qid[\"who\"].append(qid)\n",
    "    elif val_data['q'][i][0].lower() == \"where\":\n",
    "        qtype2qid[\"where\"].append(qid)\n",
    "    elif val_data['q'][i][0].lower() == \"how\":\n",
    "        qtype2qid[\"how\"].append(qid)\n",
    "\n",
    "\n",
    "q_lens = [len(q) for q in train_data['q']]\n",
    "cs_lens = [[len(c) for c in cs] for cs in train_data['cs']]\n",
    "cs_lens = list(itertools.chain(*cs_lens))\n",
    "y_lens = [len(y) for y in train_data['y']]\n",
    "photo_lens = [len(train_shared['albums'][aid]['photo_ids']) for aid in train_shared['albums']]\n",
    "all_photos_lens = [sum(len(train_shared['albums'][aid]['photo_ids']) for aid in aid_list) for aid_list in train_data['aid']]\n",
    "pts_lens = [len(pt) for aid in train_shared['albums'] for pt in train_shared['albums'][aid]['photo_titles']] #number of photos/album\n",
    "when_lens = [len(train_shared['albums'][aid]['when']) for aid in train_shared['albums']]\n",
    "album_title_lens = [len(train_shared['albums'][aid]['title']) for aid in train_shared['albums']]\n",
    "album_desc_lens = [len(train_shared['albums'][aid]['description']) for aid in train_shared['albums']]\n",
    "\n",
    "Q_THRES = int(np.percentile(q_lens, 90)) # 10\n",
    "Y_THRES = int(np.percentile(cs_lens, 90)) # 3, same as np.percentile(y_lens, 90)\n",
    "PTS_THRES = int(np.percentile(pts_lens, 90)) # 8\n",
    "WHEN_THRES = int(np.percentile(when_lens, 90)) # 4\n",
    "PHOTOS_PER_ALBUM = int(np.percentile(photo_lens, 90)) # 10\n",
    "ALBUM_TITLE_THRES = int(np.percentile(album_title_lens, 90)) # 8\n",
    "ALBUM_DESC_THRES = int(np.percentile(album_desc_lens, 50)) # 11\n",
    "\n",
    "def train_collate(batch):\n",
    "    X, Y = zip(*batch)\n",
    "    q_vec = []\n",
    "    cs_vec = []\n",
    "    desc_vec = []\n",
    "    img_feats = []\n",
    "    q_len = []\n",
    "    cs0_len = []\n",
    "    cs1_len = []\n",
    "    cs2_len = []\n",
    "    cs3_len = []\n",
    "    desc_len = []\n",
    "    img_len = []\n",
    "    qid = []\n",
    "    new_X = {}\n",
    "    for x in X:\n",
    "        q_len.append(x['q_len'])\n",
    "        cs0_len.append(x['cs_lens'][0])\n",
    "        cs1_len.append(x['cs_lens'][1])\n",
    "        cs2_len.append(x['cs_lens'][2])\n",
    "        cs3_len.append(x['cs_lens'][3])\n",
    "        desc_len.append(x['desc_len'])\n",
    "        img_len.append(x['img_len'])\n",
    "        q_vec.append(x['q_vec'])\n",
    "        #x['cs_vec'] expected shape: Y_THRES, 4, 100\n",
    "        cs_vec.append(x['cs_vec'])\n",
    "        desc_vec.append(x['desc_vec'])\n",
    "        img_feats.append(x['img_feats'])\n",
    "        qid.append(x['qid'])\n",
    "\n",
    "    new_X['q_len'] = torch.LongTensor(q_len)\n",
    "    new_X['cs0_lens'] = torch.LongTensor(cs0_len)\n",
    "    new_X['cs1_lens'] = torch.LongTensor(cs1_len)\n",
    "    new_X['cs2_lens'] = torch.LongTensor(cs2_len)\n",
    "    new_X['cs3_lens'] = torch.LongTensor(cs3_len)\n",
    "    new_X['desc_len'] = torch.LongTensor(desc_len)\n",
    "    new_X['img_len'] = torch.LongTensor(img_len)\n",
    "    new_X['q_vec'] = pad_sequence(q_vec, batch_first=False, padding_value=0)  # question \n",
    "    # expected shape: B, Y_THRES, 4, 100\n",
    "    new_X['cs_vec'] = pad_sequence(cs_vec, batch_first=False, padding_value=0) # B, Y_THRES, 4, 100 -> 4 choices T, B, 4, 100\n",
    "    new_X['desc_vec'] = pad_sequence(desc_vec, batch_first=False, padding_value=0)\n",
    "    new_X['img_feats'] = pad_sequence(img_feats, batch_first=False, padding_value=0)\n",
    "\n",
    "    return new_X, torch.LongTensor(Y), qid\n",
    "\n",
    "class MemexQA_new(Dataset):\n",
    "    def __init__(self, data, shared):\n",
    "        self.data = data\n",
    "        self.shared = shared\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['q'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        returned_item = {}\n",
    "        # self.data keys -> ['q', 'idxs', 'cy', 'ccs', 'qid', 'y', 'aid', 'cq', 'yidx', 'cs']\n",
    "        # self.shared keys -> ['albums', 'pid2feat', 'word2vec', 'charCounter', 'wordCounter']\n",
    "        returned_item['qid'] = self.data['qid'][idx]\n",
    "        q = self.data['q'][idx]\n",
    "        # missing glove word-> [0] * 100 embedding\n",
    "        q_vec = torch.FloatTensor(\n",
    "            [self.shared['word2vec'][word.lower()] if word.lower() in self.shared['word2vec'] else [0] * 100 for word in\n",
    "             q])\n",
    "        q_vec = q_vec[:Q_THRES]\n",
    "        returned_item['q_vec'] = q_vec  # largest possible shape: Q_THRES * 100\n",
    "        returned_item['q_len'] = q_vec.shape[0] \n",
    "        # choices glove\n",
    "        wrong_cs = self.data['cs'][idx]\n",
    "        correct_c = self.data['y'][idx]\n",
    "        yidx = self.data['yidx'][idx]\n",
    "        if yidx == 0:\n",
    "            cs = [correct_c] + wrong_cs\n",
    "        elif yidx == 1:\n",
    "            cs = wrong_cs[:1] + [correct_c] + wrong_cs[1:]\n",
    "        elif yidx == 2:\n",
    "            cs = wrong_cs[:2] + [correct_c] + wrong_cs[2:]\n",
    "        else:  # yidx == 3\n",
    "            cs = wrong_cs + [correct_c]\n",
    "\n",
    "        cs_vec = [\n",
    "            [self.shared['word2vec'][word.lower()] if word.lower() in self.shared['word2vec'] else [0] * 100 for word in\n",
    "             c] for c in cs]\n",
    "        cs_vec = [torch.FloatTensor(c[:Y_THRES]) for c in cs_vec]\n",
    "        cs_lens = [min(Y_THRES, len(each)) for each in cs_vec] #YTHRES\n",
    "        returned_item['cs_vec'] = pad_sequence(cs_vec, batch_first = False)  # [c1, c2, c3, c4]; largest possible shape: 4, Y_THRES, 100 ->  Y_THRES, 4, 100\n",
    "        returned_item['cs_lens'] = cs_lens\n",
    "\n",
    "        # aid: description + title , aid:when , aid : photo_titles + {later ->( photo_captions  + photo tags )}\n",
    "        aid_list = self.data['aid'][idx]\n",
    "        pts_descs = []  # photo-level\n",
    "        pid_features = []  # img features from pre-trained CNN\n",
    "        # for each album\n",
    "        total_cat_len = ALBUM_TITLE_THRES + ALBUM_DESC_THRES + WHEN_THRES + PTS_THRES  # 8 + 11 + 4 + 8 = 31\n",
    "        for aid in aid_list:\n",
    "            # ptags = {for each self.album_itags[aid]}\n",
    "            album = self.shared['albums'][aid]\n",
    "            pts = album['photo_titles']  # all photo titles/aid\n",
    "\n",
    "            # concatenate album description, album title and album when\n",
    "            desc = album['description'][:ALBUM_DESC_THRES] + album['title'][:ALBUM_TITLE_THRES] + album['when'][\n",
    "                                                                                                  :WHEN_THRES]\n",
    "\n",
    "            for pt in pts:\n",
    "                photo_info = desc + pt[:PTS_THRES]\n",
    "                # largest possible shape: total_cat_len, 100\n",
    "                photo_info_vec = [\n",
    "                    self.shared['word2vec'][word.lower()] if word.lower() in self.shared['word2vec'] else [0] * 100 for\n",
    "                    word in photo_info]\n",
    "                if len(photo_info_vec) < total_cat_len:\n",
    "                    photo_info_vec = photo_info_vec + [[0] * 100 for _ in range(\n",
    "                        total_cat_len - len(photo_info_vec))]  # total_cat_len, 100\n",
    "                pts_descs.append(photo_info_vec)  # total number of photos (varies), total_cat_len, 100\n",
    "\n",
    "            for pid in self.shared['albums'][aid]['photo_ids']:\n",
    "                # img_feats\n",
    "                pid_features.append(self.shared['pid2feat'][pid])  # total number of photos (varies) * 2537\n",
    "\n",
    "        desc_vec = torch.FloatTensor(pts_descs).view(-1,\n",
    "                                                     total_cat_len * 100)  # total number of photos (varies), total_cat_len * 100\n",
    "        returned_item['desc_vec'] = desc_vec\n",
    "        returned_item['desc_len'] = desc_vec.shape[0]\n",
    "        img_feats_vec = torch.FloatTensor(\n",
    "            pid_features)  # total number of photos (varies), 2537; NEWLY CHANGED (no matter what, it will vary; keep consistent with desc_vec)\n",
    "        returned_item['img_feats'] = img_feats_vec\n",
    "        returned_item['img_len'] = img_feats_vec.shape[0]\n",
    "        return returned_item, yidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data......\n",
      "Loading data took 11.1 seconds\n",
      "Loading prev state......\n",
      "AttentionModel(\n",
      "  (cos): CosineSimilarity()\n",
      "  (softmax1): Softmax(dim=1)\n",
      "  (softmax2): Softmax(dim=2)\n",
      "  (rnn_q): LSTM(100, 512, num_layers=3, bidirectional=True)\n",
      "  (rnn_c): LSTM(100, 512, num_layers=3, bidirectional=True)\n",
      "  (rnn_desc): LSTM(3100, 512, num_layers=3, bidirectional=True)\n",
      "  (rnn_ps): LSTM(2537, 512, num_layers=3, bidirectional=True)\n",
      "  (vis_text): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (tanh1): Tanh()\n",
      "  (CH_linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (img_linear): Linear(in_features=512, out_features=64, bias=True)\n",
      "  (tanh2): Tanh()\n",
      "  (tanh3): Tanh()\n",
      "  (last_softmax): Linear(in_features=5120, out_features=1, bias=True)\n",
      ")\n",
      "Starting training......\n",
      "TRAIN ===> Epoch 1, took time 370.7s, train accu: 0.2832, train loss: 1.374751\n",
      "TRAIN ACC  when :  0.3304042179261863\n",
      "TRAIN ACC  what :  0.3231062575836367\n",
      "TRAIN ACC  who :  0.2151748666271488\n",
      "TRAIN ACC  where :  0.20912178554099953\n",
      "TRAIN ACC  how :  0.24394366197183098\n",
      "Start validation......\n",
      "VALID ===> Epoch 0, took time 23.1s, valid accu: 0.2925, valid loss: 1.373541\n",
      "VALID ACC  when :  0.3338870431893688\n",
      "VALID ACC  what :  0.32934553131597466\n",
      "VALID ACC  who :  0.24331550802139038\n",
      "VALID ACC  where :  0.23508137432188064\n",
      "VALID ACC  how :  0.23076923076923078\n",
      "Starting training......\n",
      "TRAIN ===> Epoch 2, took time 369.8s, train accu: 0.2827, train loss: 1.373805\n",
      "TRAIN ACC  when :  0.3252534156015866\n",
      "TRAIN ACC  what :  0.3222896790980052\n",
      "TRAIN ACC  who :  0.21453900709219859\n",
      "TRAIN ACC  where :  0.2153920619554695\n",
      "TRAIN ACC  how :  0.24268018018018017\n",
      "Start validation......\n",
      "VALID ===> Epoch 1, took time 21.8s, valid accu: 0.2942, valid loss: 1.372495\n",
      "VALID ACC  when :  0.3338870431893688\n",
      "VALID ACC  what :  0.33004926108374383\n",
      "VALID ACC  who :  0.23796791443850268\n",
      "VALID ACC  where :  0.24773960216998192\n",
      "VALID ACC  how :  0.23076923076923078\n",
      "Starting training......\n",
      "TRAIN ===> Epoch 3, took time 371.6s, train accu: 0.2852, train loss: 1.372853\n",
      "TRAIN ACC  when :  0.3283318623124448\n",
      "TRAIN ACC  what :  0.32420249653259364\n",
      "TRAIN ACC  who :  0.21736562315416422\n",
      "TRAIN ACC  where :  0.21968962172647916\n",
      "TRAIN ACC  how :  0.24395727937043282\n",
      "Start validation......\n",
      "VALID ===> Epoch 2, took time 21.6s, valid accu: 0.2948, valid loss: 1.371636\n",
      "VALID ACC  when :  0.33554817275747506\n",
      "VALID ACC  what :  0.3314567206192822\n",
      "VALID ACC  who :  0.23796791443850268\n",
      "VALID ACC  where :  0.24593128390596744\n",
      "VALID ACC  how :  0.23076923076923078\n",
      "Starting training......\n",
      "TRAIN ===> Epoch 4, took time 371.1s, train accu: 0.2852, train loss: 1.371946\n",
      "TRAIN ACC  when :  0.3261348611723226\n",
      "TRAIN ACC  what :  0.3234529381175247\n",
      "TRAIN ACC  who :  0.2195266272189349\n",
      "TRAIN ACC  where :  0.22227602905569008\n",
      "TRAIN ACC  how :  0.24394366197183098\n",
      "Start validation......\n",
      "VALID ===> Epoch 3, took time 21.7s, valid accu: 0.2983, valid loss: 1.370700\n",
      "VALID ACC  when :  0.34219269102990035\n",
      "VALID ACC  what :  0.33638282899366645\n",
      "VALID ACC  who :  0.24331550802139038\n",
      "VALID ACC  where :  0.244122965641953\n",
      "VALID ACC  how :  0.23076923076923078\n",
      "Starting training......\n",
      "TRAIN ===> Epoch 5, took time 370.9s, train accu: 0.2866, train loss: 1.371064\n",
      "TRAIN ACC  when :  0.32671957671957674\n",
      "TRAIN ACC  what :  0.32611332524692427\n",
      "TRAIN ACC  who :  0.21926713947990545\n",
      "TRAIN ACC  where :  0.2217917675544794\n",
      "TRAIN ACC  how :  0.2466139954853273\n",
      "Start validation......\n",
      "VALID ===> Epoch 4, took time 22.1s, valid accu: 0.2963, valid loss: 1.369842\n",
      "VALID ACC  when :  0.33554817275747506\n",
      "VALID ACC  what :  0.3314567206192822\n",
      "VALID ACC  who :  0.23529411764705882\n",
      "VALID ACC  where :  0.24773960216998192\n",
      "VALID ACC  how :  0.2420814479638009\n",
      "Starting training......\n",
      "TRAIN ===> Epoch 6, took time 371.1s, train accu: 0.2878, train loss: 1.370247\n",
      "TRAIN ACC  when :  0.32583774250440917\n",
      "TRAIN ACC  what :  0.324488380159556\n",
      "TRAIN ACC  who :  0.22130177514792898\n",
      "TRAIN ACC  where :  0.22641509433962265\n",
      "TRAIN ACC  how :  0.2549240292628025\n",
      "Start validation......\n",
      "VALID ===> Epoch 5, took time 22.0s, valid accu: 0.2927, valid loss: 1.369968\n",
      "VALID ACC  when :  0.3338870431893688\n",
      "VALID ACC  what :  0.33286418015482055\n",
      "VALID ACC  who :  0.24598930481283424\n",
      "VALID ACC  where :  0.22603978300180833\n",
      "VALID ACC  how :  0.23076923076923078\n",
      "Starting training......\n",
      "TRAIN ===> Epoch 7, took time 370.4s, train accu: 0.2902, train loss: 1.369385\n",
      "TRAIN ACC  when :  0.3317180616740088\n",
      "TRAIN ACC  what :  0.3257588898525585\n",
      "TRAIN ACC  who :  0.22603550295857988\n",
      "TRAIN ACC  where :  0.22593130140299952\n",
      "TRAIN ACC  how :  0.25731981981981983\n",
      "Start validation......\n",
      "VALID ===> Epoch 6, took time 21.9s, valid accu: 0.2942, valid loss: 1.368591\n",
      "VALID ACC  when :  0.3338870431893688\n",
      "VALID ACC  what :  0.3335679099225897\n",
      "VALID ACC  who :  0.22192513368983957\n",
      "VALID ACC  where :  0.24050632911392406\n",
      "VALID ACC  how :  0.2420814479638009\n",
      "Starting training......\n",
      "TRAIN ===> Epoch 8, took time 370.4s, train accu: 0.2913, train loss: 1.368741\n",
      "TRAIN ACC  when :  0.33054208902600263\n",
      "TRAIN ACC  what :  0.3245538035002599\n",
      "TRAIN ACC  who :  0.2279976373301831\n",
      "TRAIN ACC  where :  0.23058252427184467\n",
      "TRAIN ACC  how :  0.26366197183098594\n",
      "Start validation......\n",
      "VALID ===> Epoch 7, took time 21.7s, valid accu: 0.2972, valid loss: 1.367743\n",
      "VALID ACC  when :  0.3338870431893688\n",
      "VALID ACC  what :  0.3406052076002815\n",
      "VALID ACC  who :  0.2192513368983957\n",
      "VALID ACC  where :  0.2423146473779385\n",
      "VALID ACC  how :  0.2420814479638009\n",
      "Starting training......\n",
      "TRAIN ===> Epoch 9, took time 370.4s, train accu: 0.2936, train loss: 1.368006\n",
      "TRAIN ACC  when :  0.3373068432671082\n",
      "TRAIN ACC  what :  0.32662968099861306\n",
      "TRAIN ACC  who :  0.22754137115839243\n",
      "TRAIN ACC  where :  0.23426911907066797\n",
      "TRAIN ACC  how :  0.2622397298818233\n",
      "Start validation......\n",
      "VALID ===> Epoch 8, took time 21.8s, valid accu: 0.2945, valid loss: 1.367292\n",
      "VALID ACC  when :  0.3338870431893688\n",
      "VALID ACC  what :  0.3370865587614356\n",
      "VALID ACC  who :  0.21657754010695188\n",
      "VALID ACC  where :  0.23688969258589512\n",
      "VALID ACC  how :  0.2420814479638009\n",
      "Starting training......\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-329336dfb101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    239\u001b[0m      \u001b[0;34m'prepro_v1.1/val_data.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prepro_v1.1/val_shared.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m      \u001b[0;34m'prepro_v1.1/test_data.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prepro_v1.1/test_shared.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m      isTrain = True)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-329336dfb101>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train_data_pth, train_shared_pth, val_data_pth, val_shared_pth, test_data_pth, test_shared_pth, isTrain)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mt_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-536a3233fd3d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mq_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_lens_unpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# M X B X 2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mq_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B X M X 2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mc0_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0_lens_unpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc0_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# T X B X 2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpad_packed_sequence\u001b[0;34m(sequence, batch_first, padding_value, total_length)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0munsorted_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mbatch_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpadded_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munsorted_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpadded_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import csv\n",
    "\n",
    "# # hyperparams\n",
    "# EPOCHS = 10\n",
    "# BATCH_SIZE = 64\n",
    "\n",
    "# # optimizer-related\n",
    "# MOMENTUM = 1e-2\n",
    "# LR = 1e-2\n",
    "# LR_STEPSIZE = 5\n",
    "# LR_DECAY = 0.85\n",
    "# WD = 5e-6\n",
    "\n",
    "# model hyperparams\n",
    "EPOCHS = 80\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3\n",
    "KERNEL = 5\n",
    "STRIDE = 1\n",
    "DROPOUT = 0.4\n",
    "\n",
    "# optimizer-related\n",
    "#MOMENTUM = 1e-2\n",
    "LR = 0.32\n",
    "WD = 5e-5\n",
    "\n",
    "# scheduler-related\n",
    "# LR_STEPSIZE = 3\n",
    "# LR_DECAY = 0.85\n",
    "FACTOR = 0.95\n",
    "PATIENCE = 3\n",
    "THRESHOLD = 0.01 \n",
    "\n",
    "def main(train_data_pth, train_shared_pth, val_data_pth, val_shared_pth, test_data_pth, test_shared_pth, isTrain):\n",
    "    cuda = torch.cuda.is_available()\n",
    "    num_workers = 8 if cuda else 0\n",
    "    print(\"Loading data......\")\n",
    "    start = time.time()\n",
    "\n",
    "    train_shared = pd.read_pickle(train_shared_pth)\n",
    "    # random initial embedding matrix for new words\n",
    "    nonglove_dict = {word: np.random.normal(0, 1, 100) for word in train_shared['wordCounter'] if word not in train_shared['word2vec']}\n",
    "    train_shared['word2vec'].update(nonglove_dict)\n",
    "    \n",
    "    val_shared = pd.read_pickle(val_shared_pth)\n",
    "    val_nonglove_dict = {word: np.random.normal(0, 1, 100) for word in val_shared['wordCounter'] if word not in val_shared['word2vec']}\n",
    "    val_shared['word2vec'].update(val_nonglove_dict)\n",
    "\n",
    "    test_shared = pd.read_pickle(test_shared_pth)\n",
    "    test_nonglove_dict = {word: np.random.normal(0, 1, 100) for word in test_shared['wordCounter'] if word not in test_shared['word2vec']}\n",
    "    test_shared['word2vec'].update(test_nonglove_dict)\n",
    "\n",
    "    train_data = MemexQA_new(data=pd.read_pickle(train_data_pth), shared=train_shared)\n",
    "    valid_data = MemexQA_new(data=pd.read_pickle(val_data_pth), shared=val_shared)\n",
    "    test_data = MemexQA_new(data=pd.read_pickle(test_data_pth), shared=test_shared)\n",
    "\n",
    "    train_loader_args = dict(shuffle=True, batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\n",
    "        else dict(shuffle=True, batch_size=BATCH_SIZE, collate_fn=train_collate)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, **train_loader_args)\n",
    "\n",
    "    valid_loader_args = dict(shuffle=False, batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\n",
    "        else dict(shuffle=False, batch_size=BATCH_SIZE, collate_fn=train_collate)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data, **valid_loader_args)\n",
    "\n",
    "    test_loader_args = dict(shuffle=False, batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\n",
    "        else dict(shuffle=False, batch_size=BATCH_SIZE, collate_fn=train_collate)  # TODO: test_collate\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, **test_loader_args)\n",
    "    print(f\"Loading data took {time.time() - start:.1f} seconds\")\n",
    "    \n",
    "    # initialize model\n",
    "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "    \n",
    "    model = AttentionModel(q_cs_input_size=100, desc_input_size = 3100, img_input_size =2537, hidden_size = HIDDEN_SIZE, batch_size = BATCH_SIZE, num_layers = NUM_LAYERS,\\\n",
    "                           device = device, img_linear_size = 64,num_choices = 4, rnn_type = 'bilstm')\n",
    "    snapshot_prefix = os.path.join(os.getcwd(), 'snapshot/')\n",
    "    PATH = snapshot_prefix + \"Model_\"+str(30)\n",
    "    print(\"Loading prev state......\")\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    model.to(device)\n",
    "    print(model)\n",
    "    # setup optim and loss\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    # setup optim and loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer= optim.Adam(model.parameters(), lr = LR, weight_decay= WD)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr = LR, weight_decay= WD)\n",
    "    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_STEPSIZE, gamma=LR_DECAY)\n",
    "\n",
    "    # training\n",
    "    for e in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        print(\"Starting training......\")\n",
    "        model.train()\n",
    "        n_correct, n_total = 0, 0\n",
    "        batch_count = 0\n",
    "        t_loss = 0\n",
    "        q_totals = [0] * 5\n",
    "        q_corrects = [0] * 5\n",
    "        for j, (batch_data, batch_labels,qids) in enumerate(train_loader):\n",
    "            if j == len(train_loader) - 1:\n",
    "                break\n",
    "            batch_labels = batch_labels.long().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_data)\n",
    "            loss = criterion(output, batch_labels)\n",
    "            t_loss += loss.item()\n",
    "            res = torch.argmax(output, 1)\n",
    "            res = res.to(device)\n",
    "            for i, qid in enumerate(qids):\n",
    "                if qid in qtype2qid[\"when\"]:\n",
    "                    q_totals[0] += 1\n",
    "                    if batch_labels[i] == res[i]:\n",
    "                        q_corrects[0] += 1\n",
    "                if qid in qtype2qid[\"what\"]:\n",
    "                    q_totals[1] += 1\n",
    "                    if batch_labels[i] == res[i]:\n",
    "                        q_corrects[1] += 1\n",
    "                if qid in qtype2qid[\"who\"]:\n",
    "                    q_totals[2] += 1\n",
    "                    if batch_labels[i] == res[i]:\n",
    "                        q_corrects[2] += 1\n",
    "                if qid in qtype2qid[\"where\"]:\n",
    "                    q_totals[3] += 1\n",
    "                    if batch_labels[i] == res[i]:\n",
    "                        q_corrects[3] += 1\n",
    "                if qid in qtype2qid[\"how\"]:\n",
    "                    q_totals[4] += 1\n",
    "                    if batch_labels[i] == res[i]:\n",
    "                        q_corrects[4] += 1\n",
    "            n_correct += (res == batch_labels).sum().item()\n",
    "            n_total += batch_labels.shape[0]\n",
    "            batch_count += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #if batch_count % 20 == 19:\n",
    "            #    print(f\"correct choice:{batch_labels[:3]} , predicted choice: {res[:3]}\")\n",
    "        train_acc = n_correct / n_total\n",
    "        train_loss = t_loss / batch_count\n",
    "        print(f\"TRAIN ===> Epoch {e + 1}, took time {time.time()-start:.1f}s, train accu: {train_acc:.4f}, train loss: {train_loss:.6f}\")\n",
    "        train_acc_q_type = np.array(q_corrects) /np.array(q_totals)\n",
    "        for i in range(5):  # 5 types of questions\n",
    "            print(\"TRAIN ACC \", q_types[i], \": \", train_acc_q_type[i])\n",
    "        #scheduler.step()\n",
    "        \n",
    "        # validate and save model \n",
    "        print(\"Start validation......\")\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            model.eval()            \n",
    "            valid_correct, loss, num_of_batches, num_of_val = 0, 0, 0, 0\n",
    "            # validation for classification\n",
    "            q_totals = [0] * 5\n",
    "            q_corrects = [0] * 5\n",
    "            for k, (vb_data, vb_label,qids) in enumerate(valid_loader):\n",
    "                if k == len(valid_loader) - 1:\n",
    "                    break\n",
    "                vb_label = vb_label.long().to(device)\n",
    "                v_output = model(vb_data)\n",
    "                resm = torch.argmax(v_output, 1)\n",
    "                resm = resm.to(device)\n",
    "                for i, qid in enumerate(qids):\n",
    "                    if qid in qtype2qid[\"when\"]:\n",
    "                        q_totals[0] += 1\n",
    "                        if vb_label[i] == resm[i]:\n",
    "                            q_corrects[0] += 1\n",
    "                    if qid in qtype2qid[\"what\"]:\n",
    "                        q_totals[1] += 1\n",
    "                        if vb_label[i] == resm[i]:\n",
    "                            q_corrects[1] += 1\n",
    "                    if qid in qtype2qid[\"who\"]:\n",
    "                        q_totals[2] += 1\n",
    "                        if vb_label[i] == resm[i]:\n",
    "                            q_corrects[2] += 1\n",
    "                    if qid in qtype2qid[\"where\"]:\n",
    "                        q_totals[3] += 1\n",
    "                        if vb_label[i] == resm[i]:\n",
    "                            q_corrects[3] += 1\n",
    "                    if qid in qtype2qid[\"how\"]:\n",
    "                        q_totals[4] += 1\n",
    "                        if vb_label[i] == resm[i]:\n",
    "                            q_corrects[4] += 1\n",
    "                correct = (resm == vb_label).sum().item()\n",
    "                valid_correct += correct\n",
    "                loss += criterion(v_output, vb_label).item()\n",
    "                num_of_batches += 1\n",
    "                num_of_val += vb_label.shape[0]\n",
    "                #if num_of_batches % 20 == 19:\n",
    "                #    print(f\"correct choice:{vb_label[:3]} , predicted choice: {resm[:3]}\")\n",
    "            val_loss = loss / num_of_batches\n",
    "            val_accu = valid_correct / num_of_val\n",
    "        print(f\"VALID ===> Epoch {e}, took time {time.time()-start:.1f}s, valid accu: {val_accu:.4f}, valid loss: {val_loss:.6f}\")\n",
    "        train_acc_q_type = np.array(q_corrects) /np.array(q_totals)\n",
    "        for i in range(5):  # 5 types of questions\n",
    "            print(\"VALID ACC \", q_types[i], \": \", train_acc_q_type[i])\n",
    "        if (e+1) % 10 == 0:\n",
    "            snapshot_prefix = os.path.join(os.getcwd(), 'snapshot/')\n",
    "            if not os.path.exists(snapshot_prefix):\n",
    "                os.makedirs(snapshot_prefix)\n",
    "            torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        #'scheduler_state_dict' : scheduler.state_dict(),\n",
    "            }, snapshot_prefix + \"Model_\"+str(i))\n",
    "\n",
    "    # testing\n",
    "    if not isTrain:\n",
    "        print(\"Start testing......\")\n",
    "        start = time.time()\n",
    "        model.eval()\n",
    "        with torch.no_grad(), open('test_predictions.csv', 'w') as f:\n",
    "            writer = csv.writer(f, delimiter=',')\n",
    "            writer.writerow([\"predict\",\"actual\"])\n",
    "            for (tbatch_data, tbatch_data_labels) in test_loader:\n",
    "                test_out = model(tbatch_data)\n",
    "                predict = torch.argmax(test_out, axis=1)\n",
    "                correct = (predict == tbatch_data_labels).sum().item()\n",
    "                for (pred, actual) in zip(predict, correct):\n",
    "                    writer.writerow([pred, actual])\n",
    "        print(f\"Testing took {time.time()-start:.1f}s\")\n",
    "    print(\"Finished\")\n",
    "                \n",
    "                                                    \n",
    "    \n",
    "main('prepro_v1.1/train_data.p', 'prepro_v1.1/train_shared.p', \n",
    "     'prepro_v1.1/val_data.p', 'prepro_v1.1/val_shared.p', \n",
    "     'prepro_v1.1/test_data.p', 'prepro_v1.1/test_shared.p',\n",
    "     isTrain = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import csv\n",
    "\n",
    "def main(train_data_pth, train_shared_pth, val_data_pth, val_shared_pth, test_data_pth, test_shared_pth, isTrain):\n",
    "    cuda = torch.cuda.is_available()\n",
    "    num_workers = 8 if cuda else 0\n",
    "    print(\"Loading data......\")\n",
    "    start = time.time()\n",
    "    train_shared = pd.read_pickle(train_shared_pth)\n",
    "    # random initial embedding matrix for new words\n",
    "    nonglove_dict = {word: np.random.normal(0, 1, 100) for word in train_shared['wordCounter'] if word not in train_shared['word2vec']}\n",
    "    train_shared['word2vec'].update(nonglove_dict)\n",
    "    \n",
    "    val_shared = pd.read_pickle(val_shared_pth)\n",
    "    val_nonglove_dict = {word: np.random.normal(0, 1, 100) for word in val_shared['wordCounter'] if word not in val_shared['word2vec']}\n",
    "    val_shared['word2vec'].update(val_nonglove_dict)\n",
    "\n",
    "    test_shared = pd.read_pickle(test_shared_pth)\n",
    "    test_nonglove_dict = {word: np.random.normal(0, 1, 100) for word in test_shared['wordCounter'] if word not in test_shared['word2vec']}\n",
    "    test_shared['word2vec'].update(test_nonglove_dict)\n",
    "\n",
    "    # train_data = MemexQA_new(data=pd.read_pickle('prepro_v1.1/train_data.p'), shared=train_shared, info=None)\n",
    "    # valid_data = MemexQA_new(data=pd.read_pickle('prepro_v1.1/val_data.p'), shared=val_shared, info=None)\n",
    "    # test_data = MemexQA_new(data=pd.read_pickle('prepro_v1.1/test_data.p'), shared=test_shared, info=None)\n",
    "    train_data = MemexQA_new(data=pd.read_pickle(train_data_pth), shared=train_shared)\n",
    "    valid_data = MemexQA_new(data=pd.read_pickle(val_data_pth), shared=val_shared)\n",
    "    test_data = MemexQA_new(data=pd.read_pickle(test_data_pth), shared=test_shared)\n",
    "\n",
    "    # random initial embedding matrix for new words\n",
    "    # config.emb_mat = np.array([idx2vec_dict[idx] if idx2vec_dict.has_key(idx) \n",
    "    # else np.random.multivariate_normal(np.zeros(config.word_emb_size), np.eye(config.word_emb_size)) \n",
    "    # for idx in xrange(config.word_vocab_size)],dtype=\"float32\") \n",
    "\n",
    "    train_loader_args = dict(shuffle=True, batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\n",
    "        else dict(shuffle=True, batch_size=BATCH_SIZE, collate_fn=train_collate)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, **train_loader_args)\n",
    "\n",
    "    valid_loader_args = dict(batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\n",
    "        else dict(shuffle=False, batch_size=BATCH_SIZE, collate_fn=train_collate)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_data, **valid_loader_args)\n",
    "\n",
    "    test_loader_args = dict(batch_size=BATCH_SIZE, num_workers=num_workers, pin_memory=True, collate_fn=train_collate) if cuda\\\n",
    "        else dict(shuffle=False, batch_size=BATCH_SIZE, collate_fn=train_collate)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, **test_loader_args)\n",
    "    print(f\"Loading data took {time.time() - start:.1f} seconds\")\n",
    "    \n",
    "    # initialize model\n",
    "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "    #model = SimpleLSTMModel(100, 64, hyperparams['batch_size'], 2, device)\n",
    "    #q_cs_input_size, desc_input_size, img_input_size, hidden_size, batch_size, num_layers, device, q_linear_size, img_linear_size, multimodal_out, kernel, stride = 1, rnn_type = 'bilstm'\n",
    "    #model = NewFusionModel(100, 3100,2537,128, 2, 2, device, 64, 64, 4, 3, 1)    \n",
    "    # model = NewFusionModel(q_cs_input_size=100, desc_input_size=3100,img_input_size=2537,hidden_size=HIDDEN_SIZE, batch_size=BATCH_SIZE, num_layers=NUM_LAYERS, \\\n",
    "    #                        device=device, q_linear_size=64, img_linear_size=64, multimodal_out=4, kernel=KERNEL, stride=STRIDE)\n",
    "\n",
    "# q_cs_input_size, desc_input_size, img_input_size, hidden_size, batch_size, num_layers, device, img_linear_size,num_choices = 4, rnn_type = 'bilstm'\n",
    "    model = AttentionModel(q_cs_input_size=100, desc_input_size = 3100, img_input_size =2537, hidden_size = HIDDEN_SIZE, batch_size = BATCH_SIZE, num_layers = NUM_LAYERS,\\\n",
    "                           device = device, img_linear_size = 64,num_choices = 4, rnn_type = 'bilstm')\n",
    "    snapshot_prefix = os.path.join(os.getcwd(), 'snapshot/')\n",
    "    PATH = snapshot_prefix + \"Model_\"+str(30)\n",
    "    print(\"Loading prev state......\")\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    model.to(device)\n",
    "    print(model)\n",
    "    # setup optim and loss\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr = LR, weight_decay= WD)\n",
    "    #optimizer= optim.SGD(model.parameters(), momentum=hyperparams['momentum'], lr = hyperparams['lr'], weight_decay= hyperparams['weight_decay'])\n",
    "    # optimizer= optim.Adam(model.parameters(), lr = LR, weight_decay= WD)\n",
    "    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_STEPSIZE, gamma=LR_DECAY)\n",
    "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=FACTOR, patience=PATIENCE, threshold=THRESHOLD,verbose=True)\n",
    "\n",
    "\n",
    "    # training\n",
    "    print(\"Starting training......\")\n",
    "    for i in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        n_correct,n_total = 0, 0\n",
    "        batch_count = 0\n",
    "        t_loss = 0\n",
    "        for j, (batch_data, batch_labels) in enumerate(train_loader):\n",
    "            batch_labels = batch_labels.long().to(device)\n",
    "            if j == len(train_loader) - 1:\n",
    "                break\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_data)\n",
    "            loss = criterion(output, batch_labels)\n",
    "            t_loss += loss\n",
    "            res = torch.argmax(output, 1)\n",
    "            res = res.to(device)\n",
    "            n_correct += (res == batch_labels).sum().item()\n",
    "            n_total += batch_labels.shape[0]\n",
    "            batch_count += 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # if batch_count % 20 == 19:\n",
    "            #     print(f\"correct choice:{batch_labels[:3]} , predicted choice: {res[:3]}\")\n",
    "        train_acc = n_correct / n_total\n",
    "        train_loss = t_loss / batch_count\n",
    "#         print(f\"There are {n_total} questions in training data.\")\n",
    "        print(f\"TRAIN ===> Epoch {i + 1}, took time {time.time()-start:.1f}s, train accu: {train_acc:.4f}, train loss: {train_loss.item():.6f}\")\n",
    "        scheduler.step(train_loss)\n",
    "        if (i+1) % 10== 0:\n",
    "            snapshot_prefix = os.path.join(os.getcwd(), 'snapshot/')\n",
    "            if not os.path.exists(snapshot_prefix):\n",
    "                os.makedirs(snapshot_prefix)\n",
    "            torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict' : scheduler.state_dict(),\n",
    "            }, snapshot_prefix + \"Model_\"+str(i))\n",
    "        \n",
    "        \n",
    "        # validate and save model \n",
    "        #print(\"Start validation......\")\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            model.eval()            \n",
    "            valid_correct, loss, num_of_batches, num_of_val = 0, 0, 0, 0\n",
    "            # validation for classification\n",
    "            for k, (vb_data, vb_label) in enumerate(valid_loader):\n",
    "                if k == len(valid_loader) - 1:\n",
    "                    break\n",
    "                vb_label = vb_label.long().to(device)\n",
    "                v_output = model(vb_data)\n",
    "                resm = torch.argmax(v_output, axis=1)\n",
    "                resm = resm.to(device)\n",
    "                correct = (resm == vb_label).sum().item()\n",
    "                valid_correct += correct\n",
    "                loss += criterion(v_output, vb_label).item()\n",
    "                num_of_batches += 1\n",
    "                num_of_val += vb_label.shape[0]\n",
    "                # if num_of_batches % 20 == 19:\n",
    "                #     print(f\"correct choice:{vb_label[:3]} , predicted choice: {resm[:3]}\")\n",
    "            val_loss = loss / num_of_batches\n",
    "            val_accu = valid_correct / num_of_val\n",
    "        print(f\"VALID ===> Epoch {i + 1}, took time {time.time()-start:.1f}s, valid accu: {val_accu:.4f}, valid loss: {val_loss:.6f}\")\n",
    "#         print(f\"There are {num_of_val} questions in validation data.\")\n",
    "        \n",
    "    # testing\n",
    "    if not isTrain:\n",
    "        print(\"Start testing......\")\n",
    "        start = time.time()\n",
    "        model.eval()\n",
    "        with torch.no_grad(), open('test_predictions.csv', 'w') as f:\n",
    "            writer = csv.writer(f, delimiter=',')\n",
    "            writer.writerow([\"predict\",\"actual\"])\n",
    "            test_correct = 0\n",
    "            loss = 0\n",
    "            num_of_batches = 0\n",
    "            num_of_test = 0\n",
    "            for (tbatch_data, tbatch_data_labels) in test_loader:\n",
    "                if k == len(test_loader) - 1:\n",
    "                    break\n",
    "                tb_label = tbatch_data_labels.long().to(device)\n",
    "                t_output = model(tbatch_data, is_train=False)\n",
    "                tresm = torch.argmax(t_output, axis=1)\n",
    "                tresm = tresm.to(device)\n",
    "                tcorrect = (tresm == tb_label).sum().item()\n",
    "                test_correct += tcorrect\n",
    "                loss += criterion(t_output, tb_label).item()\n",
    "                num_of_batches += 1\n",
    "                num_of_test += tb_label.shape[0]\n",
    "            for (pred, actual) in zip(tresm, tb_label):\n",
    "                writer.writerow([pred, actual])\n",
    "        print(f\"TEST ===> test accu: {val_accu:.4f}, test loss: {val_loss:.6f}.\")\n",
    "        print(f\"Testing took {time.time()-start:.1f}s\")\n",
    "    print(\"Finished\")\n",
    "                \n",
    "                                                    \n",
    "    \n",
    "\n",
    "# model hyperparams\n",
    "EPOCHS = 80\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3\n",
    "KERNEL = 5\n",
    "STRIDE = 1\n",
    "DROPOUT = 0.4\n",
    "\n",
    "# optimizer-related\n",
    "#MOMENTUM = 1e-2\n",
    "LR = 0.32\n",
    "WD = 5e-5\n",
    "\n",
    "# scheduler-related\n",
    "# LR_STEPSIZE = 3\n",
    "# LR_DECAY = 0.85\n",
    "FACTOR = 0.95\n",
    "PATIENCE = 3\n",
    "THRESHOLD = 0.01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data......\n",
      "Loading data took 11.4 seconds\n",
      "Loading prev state......\n",
      "AttentionModel(\n",
      "  (cos): CosineSimilarity()\n",
      "  (softmax1): Softmax(dim=1)\n",
      "  (softmax2): Softmax(dim=2)\n",
      "  (rnn_q): LSTM(100, 512, num_layers=3, bidirectional=True)\n",
      "  (rnn_c): LSTM(100, 512, num_layers=3, bidirectional=True)\n",
      "  (rnn_desc): LSTM(3100, 512, num_layers=3, bidirectional=True)\n",
      "  (rnn_ps): LSTM(2537, 512, num_layers=3, bidirectional=True)\n",
      "  (vis_text): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (tanh1): Tanh()\n",
      "  (CH_linear): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (img_linear): Linear(in_features=512, out_features=64, bias=True)\n",
      "  (tanh2): Tanh()\n",
      "  (tanh3): Tanh()\n",
      "  (last_softmax): Linear(in_features=5120, out_features=1, bias=True)\n",
      ")\n",
      "Starting training......\n",
      "TRAIN ===> Epoch 1, took time 361.4s, train accu: 0.2832, train loss: 1.374803\n",
      "VALID ===> Epoch 1, took time 21.5s, valid accu: 0.2933, valid loss: 1.373256\n",
      "TRAIN ===> Epoch 2, took time 364.7s, train accu: 0.2829, train loss: 1.373544\n",
      "VALID ===> Epoch 2, took time 22.0s, valid accu: 0.2936, valid loss: 1.372163\n",
      "TRAIN ===> Epoch 3, took time 365.0s, train accu: 0.2844, train loss: 1.372671\n",
      "VALID ===> Epoch 3, took time 21.7s, valid accu: 0.2939, valid loss: 1.371217\n",
      "TRAIN ===> Epoch 4, took time 365.4s, train accu: 0.2855, train loss: 1.371778\n",
      "VALID ===> Epoch 4, took time 21.5s, valid accu: 0.2960, valid loss: 1.370283\n",
      "TRAIN ===> Epoch 5, took time 364.8s, train accu: 0.2866, train loss: 1.370981\n",
      "Epoch     5: reducing learning rate of group 0 to 2.5600e-01.\n",
      "VALID ===> Epoch 5, took time 21.4s, valid accu: 0.2951, valid loss: 1.369577\n",
      "TRAIN ===> Epoch 6, took time 365.1s, train accu: 0.2876, train loss: 1.370185\n",
      "VALID ===> Epoch 6, took time 21.1s, valid accu: 0.2966, valid loss: 1.368524\n",
      "TRAIN ===> Epoch 7, took time 364.9s, train accu: 0.2888, train loss: 1.369466\n",
      "VALID ===> Epoch 7, took time 21.8s, valid accu: 0.2957, valid loss: 1.368316\n",
      "TRAIN ===> Epoch 8, took time 365.7s, train accu: 0.2916, train loss: 1.368753\n",
      "VALID ===> Epoch 8, took time 22.0s, valid accu: 0.2966, valid loss: 1.367168\n",
      "TRAIN ===> Epoch 9, took time 365.5s, train accu: 0.2916, train loss: 1.368398\n",
      "Epoch     9: reducing learning rate of group 0 to 2.0480e-01.\n",
      "VALID ===> Epoch 9, took time 21.6s, valid accu: 0.2966, valid loss: 1.366732\n",
      "TRAIN ===> Epoch 10, took time 364.9s, train accu: 0.2927, train loss: 1.367620\n",
      "VALID ===> Epoch 10, took time 21.8s, valid accu: 0.2981, valid loss: 1.366330\n",
      "TRAIN ===> Epoch 11, took time 364.3s, train accu: 0.2929, train loss: 1.367316\n",
      "VALID ===> Epoch 11, took time 21.9s, valid accu: 0.2998, valid loss: 1.365905\n",
      "TRAIN ===> Epoch 12, took time 364.3s, train accu: 0.2934, train loss: 1.366752\n",
      "VALID ===> Epoch 12, took time 20.7s, valid accu: 0.3051, valid loss: 1.365387\n",
      "TRAIN ===> Epoch 13, took time 363.6s, train accu: 0.2955, train loss: 1.366225\n",
      "Epoch    13: reducing learning rate of group 0 to 1.6384e-01.\n",
      "VALID ===> Epoch 13, took time 22.0s, valid accu: 0.2948, valid loss: 1.366757\n",
      "TRAIN ===> Epoch 14, took time 365.0s, train accu: 0.2961, train loss: 1.365478\n",
      "VALID ===> Epoch 14, took time 22.0s, valid accu: 0.3016, valid loss: 1.365387\n",
      "TRAIN ===> Epoch 15, took time 365.0s, train accu: 0.2941, train loss: 1.365782\n",
      "VALID ===> Epoch 15, took time 22.1s, valid accu: 0.3031, valid loss: 1.364801\n",
      "TRAIN ===> Epoch 16, took time 364.1s, train accu: 0.2953, train loss: 1.365783\n",
      "VALID ===> Epoch 16, took time 21.7s, valid accu: 0.2998, valid loss: 1.365718\n",
      "TRAIN ===> Epoch 17, took time 365.4s, train accu: 0.2972, train loss: 1.365386\n",
      "Epoch    17: reducing learning rate of group 0 to 1.3107e-01.\n",
      "VALID ===> Epoch 17, took time 21.8s, valid accu: 0.3013, valid loss: 1.364695\n",
      "TRAIN ===> Epoch 18, took time 365.0s, train accu: 0.2958, train loss: 1.365038\n",
      "VALID ===> Epoch 18, took time 22.0s, valid accu: 0.3022, valid loss: 1.364863\n",
      "TRAIN ===> Epoch 19, took time 364.6s, train accu: 0.2971, train loss: 1.364761\n",
      "VALID ===> Epoch 19, took time 22.2s, valid accu: 0.3019, valid loss: 1.364455\n",
      "TRAIN ===> Epoch 20, took time 365.3s, train accu: 0.2977, train loss: 1.364437\n",
      "VALID ===> Epoch 20, took time 21.8s, valid accu: 0.3028, valid loss: 1.364399\n",
      "TRAIN ===> Epoch 21, took time 364.4s, train accu: 0.2989, train loss: 1.364350\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0486e-01.\n",
      "VALID ===> Epoch 21, took time 21.9s, valid accu: 0.2983, valid loss: 1.366539\n",
      "TRAIN ===> Epoch 22, took time 365.6s, train accu: 0.2975, train loss: 1.364251\n",
      "VALID ===> Epoch 22, took time 22.3s, valid accu: 0.3010, valid loss: 1.364663\n",
      "TRAIN ===> Epoch 23, took time 365.8s, train accu: 0.2986, train loss: 1.363953\n",
      "VALID ===> Epoch 23, took time 21.9s, valid accu: 0.3016, valid loss: 1.365437\n",
      "TRAIN ===> Epoch 24, took time 364.9s, train accu: 0.2981, train loss: 1.363801\n",
      "VALID ===> Epoch 24, took time 21.8s, valid accu: 0.3010, valid loss: 1.364385\n",
      "TRAIN ===> Epoch 25, took time 365.0s, train accu: 0.2983, train loss: 1.363582\n",
      "Epoch    25: reducing learning rate of group 0 to 8.3886e-02.\n",
      "VALID ===> Epoch 25, took time 21.8s, valid accu: 0.3010, valid loss: 1.364251\n",
      "TRAIN ===> Epoch 26, took time 364.6s, train accu: 0.2996, train loss: 1.363418\n",
      "VALID ===> Epoch 26, took time 21.9s, valid accu: 0.2983, valid loss: 1.364718\n",
      "TRAIN ===> Epoch 27, took time 365.0s, train accu: 0.2989, train loss: 1.363096\n",
      "VALID ===> Epoch 27, took time 22.1s, valid accu: 0.2983, valid loss: 1.364957\n",
      "TRAIN ===> Epoch 28, took time 365.2s, train accu: 0.2998, train loss: 1.362963\n",
      "VALID ===> Epoch 28, took time 20.8s, valid accu: 0.3010, valid loss: 1.364283\n",
      "TRAIN ===> Epoch 29, took time 364.5s, train accu: 0.2992, train loss: 1.362984\n",
      "Epoch    29: reducing learning rate of group 0 to 6.7109e-02.\n",
      "VALID ===> Epoch 29, took time 22.2s, valid accu: 0.2981, valid loss: 1.364603\n",
      "TRAIN ===> Epoch 30, took time 365.0s, train accu: 0.2994, train loss: 1.362859\n",
      "VALID ===> Epoch 30, took time 21.9s, valid accu: 0.2963, valid loss: 1.364827\n",
      "TRAIN ===> Epoch 31, took time 364.9s, train accu: 0.2991, train loss: 1.362733\n",
      "VALID ===> Epoch 31, took time 22.3s, valid accu: 0.2981, valid loss: 1.364577\n",
      "TRAIN ===> Epoch 32, took time 365.0s, train accu: 0.3017, train loss: 1.362442\n",
      "VALID ===> Epoch 32, took time 21.8s, valid accu: 0.2986, valid loss: 1.364503\n",
      "TRAIN ===> Epoch 33, took time 366.1s, train accu: 0.3003, train loss: 1.362628\n",
      "Epoch    33: reducing learning rate of group 0 to 5.3687e-02.\n",
      "VALID ===> Epoch 33, took time 22.4s, valid accu: 0.2989, valid loss: 1.364397\n",
      "TRAIN ===> Epoch 34, took time 364.9s, train accu: 0.3010, train loss: 1.362405\n",
      "VALID ===> Epoch 34, took time 22.0s, valid accu: 0.2989, valid loss: 1.364218\n",
      "TRAIN ===> Epoch 35, took time 364.2s, train accu: 0.3017, train loss: 1.362129\n",
      "VALID ===> Epoch 35, took time 22.7s, valid accu: 0.2975, valid loss: 1.364399\n",
      "TRAIN ===> Epoch 36, took time 364.1s, train accu: 0.3006, train loss: 1.362191\n",
      "VALID ===> Epoch 36, took time 22.2s, valid accu: 0.2981, valid loss: 1.364265\n",
      "TRAIN ===> Epoch 37, took time 364.9s, train accu: 0.3003, train loss: 1.362106\n",
      "Epoch    37: reducing learning rate of group 0 to 4.2950e-02.\n",
      "VALID ===> Epoch 37, took time 22.5s, valid accu: 0.2992, valid loss: 1.364203\n",
      "TRAIN ===> Epoch 38, took time 364.7s, train accu: 0.3015, train loss: 1.361884\n",
      "VALID ===> Epoch 38, took time 21.7s, valid accu: 0.2978, valid loss: 1.364497\n",
      "TRAIN ===> Epoch 39, took time 364.9s, train accu: 0.3018, train loss: 1.361717\n",
      "VALID ===> Epoch 39, took time 21.9s, valid accu: 0.2981, valid loss: 1.364498\n",
      "TRAIN ===> Epoch 40, took time 364.8s, train accu: 0.3026, train loss: 1.361851\n",
      "VALID ===> Epoch 40, took time 22.3s, valid accu: 0.2986, valid loss: 1.364935\n",
      "TRAIN ===> Epoch 41, took time 364.5s, train accu: 0.3012, train loss: 1.361558\n",
      "Epoch    41: reducing learning rate of group 0 to 3.4360e-02.\n",
      "VALID ===> Epoch 41, took time 22.1s, valid accu: 0.2978, valid loss: 1.364284\n",
      "TRAIN ===> Epoch 42, took time 364.3s, train accu: 0.3021, train loss: 1.361576\n",
      "VALID ===> Epoch 42, took time 22.5s, valid accu: 0.2981, valid loss: 1.364280\n",
      "TRAIN ===> Epoch 43, took time 364.1s, train accu: 0.3028, train loss: 1.361374\n",
      "VALID ===> Epoch 43, took time 22.4s, valid accu: 0.2989, valid loss: 1.364329\n",
      "TRAIN ===> Epoch 44, took time 364.3s, train accu: 0.3025, train loss: 1.361300\n",
      "VALID ===> Epoch 44, took time 21.9s, valid accu: 0.2983, valid loss: 1.364251\n",
      "TRAIN ===> Epoch 45, took time 365.3s, train accu: 0.3033, train loss: 1.361332\n",
      "Epoch    45: reducing learning rate of group 0 to 2.7488e-02.\n",
      "VALID ===> Epoch 45, took time 21.9s, valid accu: 0.2986, valid loss: 1.364207\n",
      "TRAIN ===> Epoch 46, took time 364.4s, train accu: 0.3031, train loss: 1.361349\n",
      "VALID ===> Epoch 46, took time 21.9s, valid accu: 0.2989, valid loss: 1.364528\n",
      "TRAIN ===> Epoch 47, took time 364.7s, train accu: 0.3032, train loss: 1.361099\n",
      "VALID ===> Epoch 47, took time 21.9s, valid accu: 0.2989, valid loss: 1.364299\n",
      "TRAIN ===> Epoch 48, took time 365.0s, train accu: 0.3037, train loss: 1.361030\n",
      "VALID ===> Epoch 48, took time 21.9s, valid accu: 0.3001, valid loss: 1.364660\n",
      "TRAIN ===> Epoch 49, took time 364.0s, train accu: 0.3033, train loss: 1.361134\n",
      "VALID ===> Epoch 49, took time 22.7s, valid accu: 0.2995, valid loss: 1.364458\n",
      "TRAIN ===> Epoch 50, took time 364.8s, train accu: 0.3025, train loss: 1.361265\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-728785a2c49f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m'prepro_v1.1/test_data.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m'prepro_v1.1/test_shared.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         isTrain = True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-7bddf1b1bf7f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train_data_pth, train_shared_pth, val_data_pth, val_shared_pth, test_data_pth, test_shared_pth, isTrain)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mvalid_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# validation for classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvb_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvb_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main('prepro_v1.1/train_data.p',\n",
    "        'prepro_v1.1/train_shared.p',\n",
    "        'prepro_v1.1/val_data.p',\n",
    "        'prepro_v1.1/val_shared.p',\n",
    "        'prepro_v1.1/test_data.p',\n",
    "        'prepro_v1.1/test_shared.p',\n",
    "        isTrain = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
